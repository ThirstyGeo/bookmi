[
["single-missing-data-imputations.html", "Chapter 3 Single Missing data imputations 3.1 Complete cases analysis 3.2 Missing data imputation 3.3 Mean Imputation 3.4 Regression imputation 3.5 Bayesian Stochastic regression imputation 3.6 Predictive Mean Matching", " Chapter 3 Single Missing data imputations In the previous Chapter the missing data patterns and mechanisms were evaluated and discussed. Both provide information about the locations of the missing values in the dataset and the relationship of variables with missing data and with other complete variables. Historically, this was important because some imputation methods worked best with a specific missing data pattern, or assumed missing data mechanism. Nowadays, advanced imputation methods as multiple imputation can deal with almost any missing data pattern. For the missing data mechanisms, it is still important to provide a strong idea about variables that can explain the missing data. When these variables are available in the dataset these can be used to impute the missing values. We will start this Chapter with a discussion of a in most missing data situations insufficient missing data method, complete case analysis. 3.1 Complete cases analysis Complete case analysis (CCA) means that the statistical analysis is performed in the dataset, after each missing data point has been excluded. This procedure is still one of the most use missing data handling procedures (Eekhout et al. 2012) and only results in correct mean or regression coefficient estimates when the data is MCAR. However, this method can have a large impact on the precision of the statistical test results because the main drawback of using CCA is that much of the information in the dataset will be excluded. It also leads to an incorrect estimation of standard errors when the data is MCAR, MAR and MNAR (Eekhout et al. 2014). It is for that reason not recommended. There is only one situation in which CCA gives comparable results under MAR data. That is the situation when only outcome data is missing in randomized controlled trials (RCT) and observational study designs and adjustment for covariates is required (Groenwold et al. 2011). This situation gives the same results as with Multiple Imputation when the covariates are predictive of the missing data. Groenwold et al. therefore recommend the use of CCA because CCA would be more transparent compared to Multiple Imputation. Luiblinska and Rubin (2012) however, argued against this argument because the case of only outcome missing data is limited and therefore not realistic for many missing data situations where missing data is detected in more variables. This makes that CCA can be used in one rare missing data situation and that makes Multiple Imputation are more flexible procedure to use. 3.2 Missing data imputation In this Chapter, we will discuss several single imputation methods. These imputation methods will be considered by using a dataset containing information of 50 Low Back Pain (LBP) patients. Although, this dataset is small, the methods easily generalize to larger datasets with missing values in more variables. From these 50 patients, data is obtained about Pain, Tampa scale, Disability and if Radiation in the leg is present or not. The SPSS dataset is shown in (Figure 3.1) (first 15 patients are shown): Figure 3.1: SPSS dataset with missing values in the Tampa scale variable Assume that we are interested in the relationship between Pain and the Tampa scale variable. To get a first impression about this relationship we make a scatterplot. The scatterplots of the complete and incomplete datasets are displayed in (Figure 3.2 and 3.3): Figure 3.2: Relationship between the Tampa scale and Pain variables (green dots are observed and red dots are assumed to be missing data Figure 3.3: Relationship between the Tampa scale and Pain variable. Missing data are excluded The green dots represent the observed data and the red dots the missing data points. In essence, in our dataset we have the data points that are visualized in Figure 3.3. We will use this relationship to discuss what it means when we use single imputation methods. These will be discussed below. 3.3 Mean Imputation 3.3.1 Mean imputation in SPSS Mean imputation in SPSS can be applied by using three different methods. One is to first compute the mean of the variable by using Descriptive Statistics, and replace all missing values by the mean value, the other is by using the Replace Missing Values procedure under Transform and the last one is by using the Linear Regression procedure. We will start with the first. Descriptive Statistics Via Analyze -&gt; Descriptive statistics, the descriptive statistics are calculated. The results are shown in Figure3.4. Figure 3.4: Descriptive statistics of the complete data of the Tampa scale variable. The mean of the Tampa scale variable is 38.86. Replacing the missing values by the mean result in the following complete dataset (Figure 3.5). We see that all missing values have the same value, which is 38.86, i.e. the mean value of the Tampa scale variable in the dataset after the missing values have been replaced (first 15 patients are shown). Figure 3.5: Mean imputation of missing values of the Tampascale variable. The scatterplot between the Pain and the Tampa scale variable clearly shows the result of this imputation procedure, all imputed values are located at the mean value (Figure 3.6). Figure 3.6: Scatterplot between the Tampa scale and Pain variable, after the missing values of the Tampa scale variable have been replaced by the mean. Replace Missing Values procedure You can also find the Replace Missing Values dialog box via Transform -&gt; Replace Missing Values. The following window will appear (Figure 3.7). Figure 3.7: Window for mean imputation via the Replace Missing Values procedure. Now transport the Tampa scale variable to the New variable(s) window (Figure 3.8). The default imputation procedure is Mean imputation or called “Series mean”. Figure 3.8: Window for mean imputation of the Tampa scale variable. When you click on OK, a new variable is created in the dataset using the existing variable name followed by an underscore and a sequential number. The result of the procedure is shown in Figure @ref(fig:fig39. Figure 3.9: Mean imputation of the Tampa scale variable with the Replace Missing Values procedure. Linear Regression Mean imputation in SPSS is also integrated in the Linear Regression menu. This can be approached by following the route: Analyze -&gt; Regression -&gt; Linear -&gt; Options. In the Missing Values group you can choose for Replace with mean (Figure 3.10). Figure 3.10: The option Replace with mean in the Linear Regression menu. 3.3.2 Mean imputation in R We use the same Backpain dataset as above to impute the missing data in the Tampa scale variable. First, we read in the dataset in R (first 15 patients are shown). library(foreign) dataset &lt;- read.spss(file=&quot;Backpain 50 missing.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 With the following code we apply mean imputation: library(mice) # Activate the mice package to use the mice function imp_mean &lt;- mice(dataset, method=&quot;mean&quot;, m=1, maxit=1) ## ## iter imp variable ## 1 1 Tampascale imp_mean ## Class: mids ## Number of multiple imputations: 1 ## Imputation methods: ## ID Pain Tampascale Disability Radiation ## &quot;&quot; &quot;&quot; &quot;mean&quot; &quot;&quot; &quot;&quot; ## PredictorMatrix: ## ID Pain Tampascale Disability Radiation ## ID 0 1 1 1 1 ## Pain 1 0 1 1 1 ## Tampascale 1 1 0 1 1 ## Disability 1 1 1 0 1 ## Radiation 1 1 1 1 0 The information under iter and imp means that we used one step (1 iteration) to impute the missing values with the mean in one dataset, indicated by the 1 under imp. The mice function is designed to use multiple imputation. This function will be explained in more detail in Chapter 4. The following R code 3.3 can be used to extract the imputed values in the Tampa scale variable: imp_mean$imp$Tampascale ## 1 ## 2 38.86486 ## 6 38.86486 ## 9 38.86486 ## 14 38.86486 ## 21 38.86486 ## 25 38.86486 ## 27 38.86486 ## 31 38.86486 ## 35 38.86486 ## 37 38.86486 ## 44 38.86486 ## 49 38.86486 ## 50 38.86486 The completed dataset can be extracted by using the following R code (first 15 shown): complete(imp_mean) 3.4 Regression imputation 3.4.1 Regression imputation in SPSS In SPSS regression imputation can be done via the Missing Value Analysis procedures. There are two options that can be used for regression imputation, the Regression option and the EM option. This latter abbreviation stands for Expectation Maximization (EM). Because the Regression option in SPSS has some flaws in the estimation of the regression parameters (see paper von Hippel 2004) we take as an example the EM algorithm. This algorithm is a likelihood-based procedure. This means that the most likely values of the regression coefficients are estimated given the data and these regression coefficients are subsequently used to impute the missing value. This EM procedure gives the same results as first performing a normal regression analysis in the dataset and subsequently estimate the missing values from the regression equation, after the missing values have been excluded. We will compare the EM and regression procedures below. We take as an example the imputations that are used for the missing values in the Tampa scale variable by the regression of the Tampa scale on Pain. In other words, we use for this example one predictor variable for the missing data in the Tampa scale variable. With this prediction model that is used to impute the missing data in the Tampa scale variable, the Tampa scale variable becomes the outcome variable. We will use the Backpain 50 missing.sav dataset. Step 1 to 5 below explains how to apply the procedures. Step 1 From the menus choose: Analyze -&gt; Missing Value Analysis… Figure 3.11: The Missing Value Analysis dialog box. Step 2 In the main Missing Value Analysis dialog box, select the variable(s) that you want to use for the regression method and select EM in the Estimation group (Figure 3.12). Figure 3.12: EM Selection in the Missing Value Analysis window. Step 3 To specify predicted and predictor variables, click Variables. Place the Tampascale variable in the window of the Predicted variables and the Pain variable in the Predictor Variables window (Figure 3.13). Figure 3.13: Transfer of the Tampascale and Pain variables to the Predicted and Predictor Variables windows. Step 4 Click on Continue -&gt; EM and Choose for Normal in the Distribution group. Than thick Save completed data and give the dataset a name, for example “ImpTampa_EM” (Figure 3.14). Figure 3.14: Name of dataset to save the EM results in. Step 5 Then Click Continue -&gt; OK. The new dataset “ImpTampa_EM” will open in a new window in SPSS. In this dataset the imputed data of the Tampascale Variable together with the original data is stored (Figure 3.15, first 15 patients are shown). Adjust the Width and Decimals of the Tampa scale variable in the Variable View window to 4 and 3 respectively to get the same results as in Figure 3.13. Figure 3.15: Result of the EM procedure. The imputations are generated from the linear regression model of the relationship between Pain and Function. Be aware that SPSS uses as default only quantitative variables to impute the missing values with the EM algorithm. If you want to include categorical variables in the imputation model as auxiliary variables, you have to define them as scale variables. Let’s now compare the EM procedure with a two-step procedure. We first estimate the relationship between Pain and the Tampa scale variable in the dataset with linear regression after we have excluded the cases with missing values in the Tampa scale variable. Subsequently, we use the regression coefficients from this regression model to estimate the imputed values in the Tampa scale variable. We start by estimating the linear regression model with the Tampa scale variable as outcome variable (because we have to predict missing values in this variable). The Pain variable is now the independent variable. To estimate the linear regression model: From the menus choose: Analyze -&gt; Regression -&gt; Linear Figure 3.16: Linear regression analysis with the Tampa scale as the outcome and Pain as the independent variable. Transfer the Tampa scale variable to the Dependent variable window and the Pain variable to the window of the Block 1 of 1 group. Then click OK. The following coefficients will be estimated (Figure 3.17). Figure 3.17: Result of the linear regression analysis. The linear regression model can be described as: Tampascale=32.005+1.410 ×Pain When we use this regression model to impute the missing values in the Tampa scale variable and we compare them with the EM estimates, we see that the results are the same. Figure 3.18: Predictions of the missing Tampa scale values on basis of the regression model estimated in the dataset after the missing values were excluded. When we make a scatterplot of the imputations from the regression model we see that as expected the imputed values lie directly on the regression line (Figure 3.19). Figure 3.19: Relationship between the Tampa scale and the Pain variable. The Tampa scale variable is located on the y-axis. The imputed values of the Tampa scale variable (red dots) are located on the regression line that was used to generate the imputed values. The green dots are the observed data values. 3.4.2 Regression imputation in R We will now continue with the application of regression imputation in R. We use the same dataset as we did for regression imputation in SPSS above. Regression imputation can be done in R by using the R code 3.5 with as method “norm.predict”. Only the Pain variable will be used to predict the missing values in the Tampa scale variable. library(foreign) dataset &lt;- read.spss(file=&quot;Mean imputation.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 dataset &lt;- dataset[, c(&quot;Pain&quot;, &quot;Tampascale&quot;)] imp.regress &lt;- mice(dataset, method=&quot;norm.predict&quot;, m=1, maxit=1) ## ## iter imp variable ## 1 1 Tampascale imp.regress$imp$Tampascale # Extract the imputed values ## 1 ## 2 40.46554 ## 6 41.87566 ## 9 34.82506 ## 14 40.46554 ## 21 39.05542 ## 25 39.05542 ## 27 44.69590 ## 31 46.10602 ## 35 40.46554 ## 37 43.28578 ## 44 33.41494 ## 49 34.82506 ## 50 43.28578 Expectantly, this gives comparable results as the regression imputation with SPSS above. The method “norm.predict” in the mice package is a short notation of the function mice.impute.norm.predict. This function fits a linear regression model in the dataset without missing data and generates the imputed values for the Tampa scale variable by using the regression coefficients of the linear regression model. The same regression coefficients are used to predict the missing values in the Tampa scale variable as was shown in Table 3.3. The completed dataset can be extracted by using the complete function in the mice package. 3.4.3 Stochastic regression imputation Multiple Imputation accounts for the uncertainty in the estimation of imputed values. This imputation uncertainty can be accounted for by using Stochastic regression models. With Stochastic regression models the imputation uncertainty is accounted for by adding extra error variance to the predicted values that are generated by the linear regression model. Stochastic regression can be activated in SPSS via the Missing Value Analysis and the Regression Estimation option. However, the Regression Estimation option generates incorrect regression coefficient estimates (von Hippel, 2004) and will therefore not further discussed or applied in this manual. Stochastic regression models estimated using Bayesian methods (where extra error variance is also added to the regression coefficients) can be applied in SPSS by using the Multiple Imputation procedure. This is the topic of paragraph 3.5.1. We will continue with the application of stochastic regression imputation in R. 3.4.4 Stochastic regression imputation in R In R stochastic regression imputation can be done with the mice function. There are two versions available in the mice function to apply stochastic regression imputation, one that is non-Bayesian and one that is called Bayesian stochastic regression imputation. We start with non-Bayesian stochastic regression imputation. We will use only the Pain variable to predict the missing values in the Tampa scale variable. The function that mice uses for this procedure is the mice.impute.norm.nob function. We will use for the example the same dataset as above including 50 low back pain patients with missing values in the Tampa scale variable. Stochastic regression imputation is applied in R code 3.7 by using the “norm.nob” method. dataset &lt;- read.spss(file=&quot;Backpain 50 missing.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 dataset &lt;- dataset[, c(&quot;Pain&quot;, &quot;Tampascale&quot;)] imp_nob &lt;- mice(dataset, method=&quot;norm.nob&quot;, m=1, maxit=1) ## ## iter imp variable ## 1 1 Tampascale imp_nob$imp$Tampascale # Extract the imputed values ## 1 ## 2 44.85502 ## 6 43.87097 ## 9 33.90452 ## 14 40.04896 ## 21 39.04497 ## 25 37.15932 ## 27 43.99723 ## 31 46.52839 ## 35 33.45224 ## 37 41.68063 ## 44 38.18072 ## 49 37.63710 ## 50 44.12969 The completed dataset can be extracted by using the complete function in the mice package. 3.5 Bayesian Stochastic regression imputation The difference between non-Bayesian and Bayesian imputation is that to the latter imputed values also extra variation is added to the (population) regression coefficients. In other words, we use the idea that there is not one true (population) regression coefficient but that the regression coefficient as the true population parameter follows a (probability) distribution itself. This is in contrast to a frequentist idea, which assume that there is one true population parameter and that the uncertainty (by using a confidence interval) around the population parameter is interpreted as a probability statement of the result if the research would be repeatedly conducted. Bayesian methods also incorporate uncertainty in the parameters estimates of the regression model, which are the regression coefficients and the residual variance. In the context of missing data, Bayesian estimates are used to incorporate parameter uncertainty that estimate the missing values (above adding error variance). In the missing data literature, it is said that the missing values are drawn from the posterior predictive distribution, conditional on the values of other variables. This means in the context of imputation that for the parameters of the regression model, which are the regression coefficients and the error variance, the posterior distribution is estimated first. For more information about the theory of Bayesian statistics we refer to the books of Box and Tiao (1992), Enders (2012) and Gelman et al. (2004). Bayesian stochastic regression imputation can be applied in SPSS and R. We start with an explanation of Bayesian Stochastic regression imputation in SPSS, followed by an application in R. 3.5.1 Bayesian Stochastic regression imputation in SPSS Bayesian Stochastic regression imputation can be performed via the multiple imputation menu in SPSS (IBM Cooperation 2016). The Multiple Imputation procedure in SPSS, and its settings, will be discussed in more detail in the next Chapter. For now, we will show the steps that have to be taken to apply Stochastic regression imputation. Again, we will make use of the same dataset as above with missing data in only the Tampa scale variable. To generate the imputations, we will make use of the Pain variable as the only predictor variable in the imputation model (Figure 3.20, first 15 patients shown). Figure 3.20: The example dataset with missing data in the Tampa scale variable. To apply Bayesian stochastic regression imputation, we make use of Multiple Imputation procedure. That will be explained in Step 1 to 5 below. Step 1 To start the imputation procedure, Go to Analyze -&gt; Multiple Imputation -&gt; Impute Missing Data Values. The first window that appears is the Variables window. In this window you can define which variables you want to include in the imputation model, which are the Tampascale and Pain variables. The number of imputed datasets you want to generate, which you have to set to 1, and the name of the dataset where you want to copy your imputed Tampascale values to. We give this dataset the name “ImpStoch_Tampa”, as in Figure 3.21. Figure 3.21: The Variables window. Step 2 In the Methods window you have two options for the Imputation Method. The Automatic and the Custom method. We would not advise you to use the automatic procedure. With this procedure the data will be scanned by SPSS and a monotone imputation method will be used if the data shows a monotone pattern of missing values (see paragraph on missing data patterns). Otherwise, fully conditional specification is used. You can better customize your imputation method so that you are sure the best procedure for your data will be used. We will use the Custom option and choose for fully conditional specification (MCMC). Set the Maximum iterations number at 50. This specifies the number of iterations, that is part of the FCS method (Figure 3.22). We further use the default settings. Figure 3.22: The Methods window. Step 3 In the constraints window (Figure 3.23) you can scan the data and put constraints on the minimum and maximum values of the imputed variables. It is also possible to constrain the variables in the imputation model. This will further be discussed in the next Chapter. For now, we scan the data and use for other settings the default setting. Figure 3.23: Stochastic regression imputation Step 4 In the Output window we only use the default settings. Figure 3.24: The Output window. Step 5 After you click on OK the imputation procedure will start. The output dataset consists of the original data with missing data plus a set of cases with imputed values for each imputation. These imputed datasets are stacked under each other. The file also contains a new variable, Imputation_, a numeric variable that indicates the number of the imputed dataset (0 for original data and &gt; 0 for the imputed datasets). Figure 3.25 and Figure 3.26 are the imputed datasets. In Figure 3.25 the variable Imputation_ is added to the dataset and in Figure 3.26, the imputed values are marked yellow. Figure 3.25: Imputed dataset with the variable Imputation_ to distinguish the original and imputed datasets. Figure 3.26: Imputed dataset with the imputed values marked yellow. When we make a scatterplot of the Pain and the Tampascale variable (Figure 3.27) we see that there is more variation in the Tampascale variable, or you could say that the variation in the Tampascale variable is “repaired”. Figure 3.27: Scatterplot of the relationship between Tampascale and the Pain variable, including the imputed values for the Tampascale variable (red dots). 3.5.2 Bayesian Stochastic regression imputation in R The package mice also include a Bayesian stochastic regression imputation procedure. This method also accounts for imputation uncertainty, not only via the residuals (error variance), but also via the regression coefficients, which means that extra noise (or random error) is added to the regression coefficients. This imputation procedure is done by using the mice.impute.norm function. For the example to apply this method in R code 3.8 we use the same dataset as above with missing values in the Tampa scale variable and the pain variable as the only predictor variable for these missing values. dataset &lt;- read.spss(file=&quot;Backpain 50 missing.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 dataset &lt;- dataset[, c(&quot;Pain&quot;, &quot;Tampascale&quot;)] imp_b &lt;- mice(dataset, method=&quot;norm&quot;, m=1, maxit=1) ## ## iter imp variable ## 1 1 Tampascale imp_b$imp$Tampascale # Extract the imputed values ## 1 ## 2 43.96120 ## 6 37.92496 ## 9 32.37489 ## 14 42.62826 ## 21 41.23268 ## 25 39.06396 ## 27 46.79016 ## 31 44.81496 ## 35 46.89151 ## 37 42.52979 ## 44 35.68837 ## 49 36.09117 ## 50 44.24113 The completed dataset can be extracted by using the complete function in the mice package. 3.6 Predictive Mean Matching Predictive mean matching (PMM) is an imputation method that predicts values and subsequently selects observed values to be used to replace the missing values. It is the default imputation procedure in the mice package to impute missing data in continuous variables (Rubin, 1987, p. 168). It is applied in combination with stochastic regression imputation. 3.6.1 A data example of Predictive Mean Matching We use the LBP dataset and the Tampa scale and Pain variables, with missing data in the Tampa scale variable and complete data in the Pain variable. We will show how PMM works for the first missing value in the Tampa scale variable. The first missing value is located in ID 2, i.e. in the second row and is marked white (R code 3.9) dataset &lt;- read.spss(file=&quot;Backpain 50 missing.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 dataset &lt;- dataset[, c(&quot;ID&quot;, &quot;Pain&quot;, &quot;Tampascale&quot;)] head(dataset, 15) ## ID Pain Tampascale ## 1 1 9 45 ## 2 2 6 NA ## 3 3 1 36 ## 4 4 5 38 ## 5 5 6 44 ## 6 6 7 NA ## 7 7 8 43 ## 8 8 6 43 ## 9 9 2 NA ## 10 10 4 36 ## 11 11 5 38 ## 12 12 9 47 ## 13 13 0 32 ## 14 14 6 NA ## 15 15 3 34 At the first step a simple linear regression analysis is applied in the complete cases (i.e. complete case analysis), with the Tampa scale variable as the outcome and Pain as the predictor variable. By using this model, all observed Tampa scale values are replaced by the predicted values using the intercept, the regression coefficient for Pain and the Pain values of the observed patients. This is the same as applying simple regression imputation, where the predicted values are taken from the regression line. We call these values the Predictedobserved values. Subsequently, all missing values of the Tampa scale variable are replaced by values that are generated by using Bayesian stochastic regression imputation and these values are called the Predictedmissing values. These values are predicted according to the same steps as for simple linear regression, but now Bayesian versions of the regression coefficients for Pain are used to generate the predicted values of the Tampa scale variable. The Predictedobserved and the Predictedmissing values for the Tampa scale variable are shown as the Tampascale_Pred variable in R code 3.10. R code 3.10 shows the original dataset with the Pain and Tampa scale variables, the Tampascale_Pred variable and a group variable, to indicate if a Tampa scale value is missing (Group=1) or not (Group=0). At the next step the first value that is predicted for the first missing value of ID 2 (the value 40.55550 under Tampascale_Pred) is subtracted from all observed predicted values. These differences can be found under the column that is called d. In this column the differences between the Predictedobserved values and the value 40.55550 (first predicted value for the first missing value of the Tampa scale variable) can be found, i.e. 44.69636 - 40.55550 = 4.14086. This d value is used to select the donors that contain the smallest difference between the Predictedobserved and the Predictedmissing values for the Tampa scale and that will be used to replace the missing values. In our example the smallest difference is 0.09006. Finally, one of these donors is selected randomly and the observed Tampa scale value that is related to that donor is used to replace the first missing value in the Tampa scale variable. In our example the first missing value in the Tampascale variable, when we would use 3 donors to randomly draw from, would get the value 42, 37 or 40. This procedure is repeated for every missing value. To summarize, the following steps are followed by PMM to replace missing values: Predicted values of observed Tampa scale values are estimated by using the Pain variable as predictor and the regression coefficients estimated in the observed data (complete case analysis). In other words, only the observed data is used to generate predicted Tampa scale values. These values are the Predictedobserved values. This regression model can be described by the red regression line in Figure 3.21. From the regression coefficients under 1, draws are taken from the Bayesian posterior distribution to get Bayesian estimates and these are used to generate predicted values for the missing Tampa scale values by using the Pain variable as predictor. These are the Predictedmissing values and can be derived from the blue regression line in Figure 3.21. From all Predictedobserved values, each Predictedmissing value is subtracted, i.e. starting with the first Predictedmissing value to replace the first missing value. The first value that is subtracted is the value 40.5555, which is marked white in R code 3.9 above. That subtraction results in the values under the column named d. These differences in the d column are sorted from low to high. The three cases with the lowest values are than matched to the observed values of the Tampa scale variable. These are recognizable by the numbers 1, 2 and 3 in the R code 3.10 above. To prevent duplicate values, a small value is added to these values (not shown). Than from these cases a random selected value of the observed Tampa scale variable is drawn, i.e. from the values 42, 37 and 40 when 3 donors are used. Finally, one value of these observed values is randomly chosen and used to replace the missing value in the Tampa scale variable. This process is repeated for each case with a missing value in the Tampa scale variable. Note that matching of donors is done using the predicted values (of the observed and missing values) of the variable with missing data, and not on the observed values. The observed values are used to fill in the missing values. "]
]
