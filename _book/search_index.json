[
["index.html", "Applied Missing data analysis with SPSS and R(Studio) Foreword 0.1 Software 0.2 Notation in this book 0.3 Acknowledgement", " Applied Missing data analysis with SPSS and R(Studio) Martijn Heymans and Iris Eekhout 2018-12-07 Foreword In the past decades, the attention for missing data has grown and so has the need for researchers to apply suitable methods to deal with their missing data. Leading methodologists and statisticians have published papers in leading journals about the problems of missing data and recommended researchers to take missing data seriously ((Sterne et al. (2009), Little et al. (2012), Li P (2015))). From our experience, researchers still find it difficult to reserve time to evaluate the missing data in their study and to find a suitable solution to handle their missing data for their main data analysis. This book is developed for applied researchers who are looking for a solution for their missing data problem or who want to learn more about dealing with missing data. The book is initially developed for a missing data course for epidemiologist, but we feel that applied researchers from other disciplines may also find this book useful. Further, we are active in giving statistical advice in general and more specific about missing data. Because our time for consultations is mostly limited, this practical guide may be useful to help researchers get started with their missing data problem. With this book we provide an overview of the currently available methods to deal with missing data. The use of the methods are thoroughly explained and supported by practical examples. We hope you will enjoy this book and that you find it useful, at that as a result you will use the recommended methods to solve your missing data problem. 0.1 Software In this book the software packages SPSS and R play a central role. The combination of these two software packages may seem random, but it is not. SPSS is one of the most popular software package, worldwide, to do statistical data analyses. Currently, R is growing in popularity fast and will probably become one of the most popular software packages to do data analysis, also for applied researchers. Both SPSS and R have their own specific characteristics that may some find an advantage and some a disadvantage. One of the main differences between SPSS and R is that SPSS works with a click-menu that makes windows appear. In these windows you can drag variables into boxes. Subsequently, you can click the OK button and the statistical analysis procedure you specified will run and the output results are automatically presented in a new window. A limitation of working with SPSS may be that you are overloaded with statistical output that may not all be needed to answer your research question. Also, newly developed methods are not immediately available, but have to be included in a new version update by the developers. R is a software package that needs code to access statistical procedures. In R you need to be familiar with some R-language in order to use it. However, once you know the language, R can be used for any statistical procedure you can think of. R is a free open source program see anybody can download and use it and it enables users to get insight into the calculations that are programmed by applying procedures. Anyone can contribute by developing packages and functions for statistical procedures. Consequently, state-of-the-art statistical methods are very quickly available in R, and if they are not, you can develop them yourself. In this book the handling of missing data is the main topic. We will show how to apply methods in both software packages SPSS and R. The multiple imputation methods that are discussed run with random starting procedures. Both SPSS and R use their own internal random number generators. Because these are different, results might slightly differ between the software packages, even when exactly the same function is applied. Our intention is not to compare the software packages, both packages are reliable, but we aim to provide readers with an overview of all possible methods in different missing data situations. In this book we use SPSS version 24 and R software version 3.5.1. The R examples will be presented by using the output from RStudio version (Version 1.1.463 – © 2009-2018 RStudio, Inc). RStudio is an integrated development environment (IDE) for R. RStudio includes a wide range of productivity enhancing features and runs on all major platforms. 0.2 Notation in this book Annotation in this book The name of R packages and libraries are used as they are published under their original names. The name of R functions is in italic form, like the mice function. R code of the procedures used in the book is marked grey. Book examples in R code language can be found in grey text boxes, for example: to read in the dataset Backpain 50 missing.sav. # Activate the foreign package and read in the SPSS dataset library(foreign) dataset &lt;- read.spss(file=&quot;data/Backpain 50 missing.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 0.3 Acknowledgement We first want to thank our students and colleagues in the hospital for their interesting questions about missing data during lectures and statistical consultations. The book is written with the R Bookdown package and R Markdown. We thanks the developers for making these awsome packages freely available. References "],
["software-applications.html", "Chapter 1 Software applications 1.1 SPSS 1.2 R and RStudio", " Chapter 1 Software applications We start the book with an introduction into the software packages SPSS and RStudio that are needed to conduct the procedures explained in the other chapters. If you are an user of SPSS or RStudio you can skip this Chapter without losing relevant information and continue with the next chapter. 1.1 SPSS 1.1.1 Data and Variable View windows In this book we work with SPSS version 24 (IBM, 2016). When you open SPSS you see an empty Data View window, you are than in the SPSS Data Editor window. This window is always open when you start SPSS. In the SPSS Data Editor, you have the possibility to go to the Data View and Variable View windows. In the Data View window, you can enter data yourself or read in data by using the options in the file menu. In (Figure 1.1) you see an example of a dataset in the Data View window. Each row in the Data View window represents a case and in the columns you find the variable names. In the Data View window, you can start all kind of data manipulations by using the different menu’s above in the window. Figure 1.1: Data View window in SPSS In the lower left corner of the window you can click on the tab Variable View and the Variable view window will appear (Figure 1.2). Figure 1.2: Variable View window in SPSS In the Variable View window, you can add new variables, by entering the name in the name column. Further, you can change variable options as: Type: e.g. numeric or string variable; Width: number of digits; Decimals: the number of decimal places displayed; Label: add some extra information about the type of information in the variable; Values: To assign numbers to the categories of a variable; Missing: you can define specified data values as user-missing or system missing; Columns: To change the number of characters displayed in the Data View window; Align: to specify the alignment of the data; Measure: to specify the level of each variable, scale (continuous), ordinal or nominal; Role: Here you can define the role of the variable during your analysis. Examples are, Input for independent variable, Target for dependent or outcome variable, Both, independent and dependent variable. There are more possibilities, but most of the times you use the default Input setting. 1.1.2 Analyzing data in SPSS All statistical procedures in SPSS can be found under the Analyze button (Figure 1.3). Here you also find the option “Multiple Imputation” which plays an important role in this manual. Figure 1.3: Statistical procedures that can be found under the Analyze menu in SPSS 1.1.3 The Output window in SPSS If you have run your analyses in SPSS, an SPSS Output (or viewer) Window will pop-up. The main body of the Output Window consists of two panes (left and right panes). In the left pane you find an outline of the output. In the right pane younfind the actual output of your statistical procedure (Figure 1.4). Figure 1.4: Part of the Output or Viewer window in SPSS after making use of Descriptive Statistics under the Analyze menu 1.1.4 The Syntax Editor in SPSS In the syntax editor of SPSS, you use the SPSS syntax programming language. You can run all SPSS procedures by typing in commands in this syntax editor window, instead of using the graphical user interface, i.e. by using your mouse and clicking on the menu´s. You can get access to the syntax window in two ways. The first is just by opening a new syntax file by navigating to File -&gt; New -&gt; Syntax. This will open a new syntax window (Figure 1.5). Figure 1.5: Screenshot of new syntax file You can also generate syntax by accessing statistical procedures through the dropdown menus and clicking the Paste button instead of clicking the OK button after you have specified the options. Than a new Syntax Editor window will pop up or the new syntax will automatically be added to the open Syntax Editor window. An example can be found in Figure 1.6, where the syntax is shown for the Descriptive Statistics procedure of Figure 1.4. Figure 1.6: Screenshot of Syntax editor of SPSS including the Syntax code for descriptive statisitcs In this manual we will not use SPSS syntax code to access statistical procedures. SPSS is most frequently used via the graphical user interface, and we will use that method also in this manual. 1.1.5 Reading and saving data in SPSS You can Read data in, in SPSS via the menu File: File -&gt; Open -&gt; Data. All kind of file types can be selected. Of course the SPSS .sav files, but also .por, .xlsx, .cvs, SAS, Stata, etc. (Figure 1.7)). After you have selected a specific file type other than SPSS you may have to go through several steps before you see the data in the Data View window. These steps are not necessary for SPSS files, they open directly in the data editor. Figure 1.7: Window to read in different file types in SPSS Saving files in SPSS is possible via the Save Data As option under the menu File. You can choose the same kind of file types. 1.2 R and RStudio RStudio is an integrated environment to work with the software program R. Consequently, to work with RStudio, R has to be installed. RStudio uses the R language and is also freely available. In this manual we will only show some possibilities and options in RStudio that are needed to run the R code and the programs that are discussed in this manual. For more information about RStudio and its possibilities visit the RStudio website at www.rstudio.com. When you open RStudio the following screen will appear. Figure 1.8: First screen that appears after you have started RStudio There are three windows opened: On the left is the Console window This is the main window to run R code (see below for more information about the Console window). Right above is the window where you can choose between the Environment and History tabs (e.g. history tracks the code you typed in the Console window). At the right site below is the window where you can choose between Files, Plots, Packages, Help and Viewer tabs. 1.2.1 The role of the Console Window When you enter code in the Console window you will directly receive a result. For example, when you type 3 + 3 the result will appear directly. 3 + 3 ## [1] 6 Other multiplication procedures as divide, square, etc. can also be executed. The main use for R is its functions. For example, to generate 20 random numbers you use the following function code (we will discuss more about functions in R later): rnorm(20) ## [1] -0.5105789 -1.2869735 0.4500069 0.7273155 -0.5333891 -0.7829890 ## [7] 1.3056728 0.8616403 1.4966118 1.1784637 -0.7760966 -0.6288660 ## [13] -0.7142100 0.6683671 0.3287871 -0.7273431 0.1008961 -0.1127748 ## [19] 1.3417950 -1.5670583 The number [1] between brackets is the index of the first number or item in the vector. 1.2.2 R assignments and objects In R it is possible to create objects and to assign values to these objects. In this way it is possible to store some intermediate results and recall or use them later on. Assigning values to objects is done by using the assignment operator &lt;- . You can also use the = sign as an assignment operator. This is not recommended because this is also a symbol used for mathematical operations. For example, when we want to assign the value 3 to the object x, we use: x &lt;- 3 When we subsequently type in the letter x we get the following result: x ## [1] 3 Now the value 3 is assigned to the object x. In R all kind of information can be assigned to an object, i.e. one number, a vector of numbers, results from analysis or other R objects such as data frames, matrices or lists. Objects can have all kinds of different names, composed of different letters and numbers. Here are some examples where number 3 is assigned to different objects with different names: test &lt;- 3 test.1 &lt;- 3 test.manual &lt;- 3 test ## [1] 3 test.1 ## [1] 3 test.manual ## [1] 3 Note that some letters and words are used by R itself. It is not recommended to use these leters as names for objects in R that you create yourself. For example, the letter T and F are used as TRUE and FALSE by R. Other letters that are already in use are c, q, t, C, D, I and diff, df, and pt. 1.2.3 Vectors, matrices, lists and data frames Vectors A vector can be created by the following code: y &lt;- c(1, 2, 3, 4, 5) y ## [1] 1 2 3 4 5 The numbers 1, 2, 3, 4 and 5 are assigned to the data vector y. The “c” in the above code stand for concatenate which makes that all separate (one-vector) numbers are merged into one vector. It is also possible to create character vectors, which are vectors that contain strings (text). An example: y &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;test&quot;) y ## [1] &quot;a&quot; &quot;b&quot; &quot;test&quot; Vectors can also be made by using the “:” symbol. With that symbol it is easy to generate a sequence of numbers. An example: y &lt;- 1:10 y ## [1] 1 2 3 4 5 6 7 8 9 10 Matrix You can create a matrix by using the matrix function. matrix(c(1, 2, 3, 4, 5, 6), nrow=2, ncol=3) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 Now we have created a matrix with 2 rows and 3 columns. In essence we converted the vector c(1, 2, 3, 4, 5, 6) into a matrix. List Another object is called a list. A list can contain components of different formats. Let’s look at an example using the following code: x &lt;- 1:5 x ## [1] 1 2 3 4 5 y &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;test&quot;) z &lt;- list(x=x, y=y) z ## $x ## [1] 1 2 3 4 5 ## ## $y ## [1] &quot;a&quot; &quot;b&quot; &quot;test&quot; With this code you created the list object z consisting of the two components x and y which are the vectors that were created above. You see that in a list two components of different data type can be combined, a numeric and a character factor. The names of the list components are indicated by the dollar sign, $. The list component can be obtained separately by typing z$x for component x or x$y for component y. Dataframe Mostly we work with datasets that contain information of different variables and persons. In R such a dataset is called a dataframe. Typically, a dataframe is created by reading in an existing dataset. How to create a dataframe by reading in a dataset will be further discussed in the paragraph “Reading in and saving data”. 1.2.4 Indexing Vectors, Matrices, Lists and Data frames Vectors An important operation in R is to select a subset of elements of a given vector. This is called indexing vectors. This subset can be assigned to another vector. For example: y &lt;- c(3, 5, 2, 8, 5, 4, 8, 1, 3, 6) y[c(1, 4)] ## [1] 3 8 The R code y[c(1, 4)], extracts the first and fourth element of the vector. Another example is by using the “:” symbol, to extract several subsequent elements: y[2:5] ## [1] 5 2 8 5 The R code y[2:5], extracts the second to the fifth element of the vector. A minus sign excludes the specific element from the vector, like: y[-3] ## [1] 3 5 8 5 4 8 1 3 6 The R code y[-3], excludes the third element of the vector (i.e. 2). A new vector z can be created where the third and fourth element of the y vector are excluded: z &lt;- y[-c(3, 4)] z ## [1] 3 5 5 4 8 1 3 6 Matrices When we index matrices we can choose to index rows, columns or both. Here are some examples: First construct the matrix z: z &lt;- matrix(1:9, nrow=3) z ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 Extract from the first row the number in the second column z[1, 2] ## [1] 4 Extract all numbers in each column in the first row z[1, ] ## [1] 1 4 7 Extract all numbers in each row of the first column z[, 1] ## [1] 1 2 3 A minus sign can also be used to delete specific elements or complete rows or columns. You can omit the first row from the matrix z z[-1, ] ## [,1] [,2] [,3] ## [1,] 2 5 8 ## [2,] 3 6 9 Lists First create a list with 3 components, each component consists of a vector of the same length and with 10 elements each. k &lt;- list(a=1:10, b=11:20, c=21:30) k ## $a ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## $b ## [1] 11 12 13 14 15 16 17 18 19 20 ## ## $c ## [1] 21 22 23 24 25 26 27 28 29 30 Index the individual component b by using the following code: k$b ## [1] 11 12 13 14 15 16 17 18 19 20 k[[&quot;b&quot;]] ## [1] 11 12 13 14 15 16 17 18 19 20 k[[2]] ## [1] 11 12 13 14 15 16 17 18 19 20 To extract individual list components double square brackets are used, compared to single brackets for indexing vectors and matrices. When you use single brackets, you get the following results: k[&quot;b&quot;] ## $b ## [1] 11 12 13 14 15 16 17 18 19 20 The difference between using single and double brackets is that single brackets return the component data type, which is in this case a vector (but could be any kind of data type) and single brackets always return a list. Data frames Indexing data frames follows the same method as indexing matrices, but we can also use the method that is used to index lists, since data frames are essentially lists of vectors of the same length. Data frames consist of rows and columns which can be accessed separately or both to extract specific elements. We use the data frame that was constructed in R code 1.11. z ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 To index the first column you use: z[, 1] ## [1] 1 2 3 k$a ## [1] 1 2 3 4 5 6 7 8 9 10 The second row can be accessed by using: z[2, ] ## [1] 2 5 8 1.2.5 Vectorized Calculation With R it is possible to perform vectorized calculations. This means that you can do calculations elementwise, i.e. the same calculation is done on each element of an object. Let’s look at an example. First create a vector z with numerical variables. z &lt;- c(1, 2, 3, 4, 5, 6, 7, 8) z ## [1] 1 2 3 4 5 6 7 8 Now you can fairly easy square each element of the vector by typing: z^2 ## [1] 1 4 9 16 25 36 49 64 We see that each element is squared. You can do these vectorized calculations by using all kinds of mathematical functions, e.g. taking the square root, logarithms, adding constant values to each element, etc. 1.2.6 R Functions Functions play an important role in R. Functions can be seen as lines of R code to run all kind of data procedures and to return a result. Take for example the small function “print.sum.test” that generates a sequence of numbers from 1 to 5 and sums all values from 1 to the value we define beforehand, which is here the value 3. This means that the function will sum all values from 1 to 3, i.e. 1 + 2 + 3 when it is at the value of 3 and all values from 1 to 4 when we use as input value the value 4. print.sum.test &lt;- function(x) { for (i in 1:5) { if (i==x) y &lt;- sum(1:i) } return(y) } print.sum.test(3) ## [1] 6 At the first line we define the function print.sum.test that includes one argument x. Than in de body of the function which starts with a left brace and ends with another brace, the actual calculations take place. The return statement gives back the result. At the end we call the function with the statement print.sum.test(3), where the value 3 is actually used in the function. Once the function is defined, you can call the function and plug in other values. For example: print.sum.test(4) ## [1] 10 You can write functions yourself but in R many functions are available which means that many calculations are done by using function calls. A function name is followed by a set of parentheses which contain some arguments. In the above self-written function, we already made use of a function, which is the sum function. We can use it separately as follows: sum(3,4) ## [1] 7 The arguments in this function are the numbers 3 and 4 and the result is their sum. This function uses as arguments numbers or complete vectors. If you want to see the formal arguments of each function you can use the args function. For example, you can use it for the matrix function: args(matrix) ## function (data = NA, nrow = 1, ncol = 1, byrow = FALSE, dimnames = NULL) ## NULL As a result the arguments of the functions are listed with their default settings. In this case the arguments are: data: an optional data vector (including a list or expression vector). nrow: the desired number of rows. ncol: the desired number of columns. byrow: logical. If FALSE (the default) the matrix is filled by columns, otherwise the matrix is filled by rows. dimnames: A dimnames attribute for the matrix: NULL or a list of length 2 giving the row and column names respectively. An empty list is treated as NULL, and a list of length one as row names. The list can be named, and the list names will be used as names for the dimensions. 1.2.7 The Help function There are several possibilities to start the help facilities in R and to get more information about functions and their arguments in R. You can just type help or use the question mark as follows: help(matrix) ?matrix In both ways the help tutorial for the matrix function will be activated and appear in your web browser or in help tab in the right corner below when you use R studio. 1.2.8 Working with script files If you want to use R code and functions more than once it is useful to work with scripts files. In this way you can type and save R code and reuse it. To create a script file in R is easy. After you have started RStudio you go to File -&gt; New File -&gt; R Scripts A new window will open on the left side above. In that file you can type for example the self-written function print.sum.test. (Figure 1.9). You can save the script file by using the Save option under the File menu, open it again and use it whenever you want. Figure 1.9: Script file example in RStudio 1.2.9 Creating a working directory It is a good idea to keep your R files at the same place when you are doing data analysis for some kind of project. If you do not use a separate directory, R will use a default directory, that will mostly be in Windows in the documents folder. All files that you have to use or save during your R session are in that directory. To locate your working directory, you can type in the Console window: getwd() You can change the working directory in another way. Go in RStudio to the window on the right site below and go to the Files tab and click on the right site of the screen on the three dots. Than a window will open and you can browse to your preferred folder. Then choose for More in the Files tab and then select “Set As Working Directory” (Figure 1.10). Now you have set your preferred working directory. You can check if your directory is set correctly by choosing “Go To Working Directory”. Figure 1.10: Working directory selection in RStudio 1.2.10 Reading in SPSS data in RStudio There are several procedures in RStudio to read in datasets. Import datasets via “Import Dataset” An easy way is via the window at the right site above. There you will find the button “Import Dataset”. When you click on it you can choose between different kind of file types, i.e. CVS, Excel, SPSS, SAS and Stata files (Figure 1.11)). Figure 1.11: Screen to import datasets in RStudio Here we choose for SPSS. When you apply this procedure for the first time RStudio asks for your permission to download a package called “haven”. This package is built to import and export data from several types. Using the foreign package Another way to import an SPSS dataset is by making use of the foreign package. You find this package under the Packages Tab in the window at the right site below under the heading “System Library”. Install that package first. The foreign package includes the function “read.spss”. When you are in the correct working directory, i.e. the working directory where the file that you want to import is stored, you use the following code to import the dataset: library(foreign) dataset &lt;- read.spss(file= &quot;data/Backpain50.sav&quot;, to.data.frame = T) The SPSS dataset is now connected to the object dataset. 1.2.11 Saving and Reading R data in RStudio Datasets can be saved and read in R using different commands. Write.table You can use the write.table function to save matrices and data frames (datasets): library(foreign) dataset &lt;- read.spss(file= &quot;data/Backpain50.sav&quot;, to.data.frame = T) write.table(dataset, file=&quot;data/Backpain50 R file&quot;) Files that are saved with write.table can be easily imported in SPSS by using the steps that will be discussed in the next paragraph. Read.table You can use the read.table function to read in matrices and data frames by using: dataset &lt;- read.table(file=&quot;data/Backpain50 R file&quot;) Save You can also use the command save to save datasets, according to (notice the .RData extension): library(foreign) dataset &lt;- read.spss(file= &quot;data/Backpain50.sav&quot;, to.data.frame = T) save(dataset, file=&quot;data/Backpain50 R file.RData&quot;) You can also use save without the .Rdata extension: library(foreign) dataset &lt;- read.spss(file= &quot;data/Backpain50.sav&quot;, to.data.frame = T) save(dataset, file=&quot;data/Backpain50 R file&quot;) To get direct access to the data that you have saved, you can use the get function in combination with the load function like this: library(foreign) dataset &lt;- read.spss(file= &quot;data/Backpain50.sav&quot;, to.data.frame = T) save(dataset, file=&quot;data/Backpain50 R file&quot;) get(load(file=&quot;data/Backpain50 R file&quot;)) With save, you can save any R object, also lists such as: x &lt;- list(a=1, b=&quot;example&quot;, c=3) save(x, file=&quot;data/listsave.RData&quot;) Load You can Load dataset or lists again in the workspace by using: load(file=&quot;data/Backpain50 R file.RData&quot;) or: load(file=&quot;data/listsave.RData&quot;) 1.2.12 Reading in (R)Studio data into SPSS When you have used the write.table function to save data in R you can easily read them in into SPSS. Before you can read in the dataset in SPSS you have to use write.table in the following way: write.table(dataset, file=&quot;data/Backpain50 R file&quot;, sep=&quot;;&quot;, dec=&quot;,&quot;, row.names=F) The extra parameter settings, mean: sep=“;”, separate each variable by an “;” indicator. dec=“,”, use for decimals a “,” instead of an “.”. row.names=F , Do not add an extra column with row.names. Follow the next steps to read in the data into SPSS: File -&gt; Open data -&gt; “All files (.)” than you will see the file you want to import in SPSS, here the “Backpain50 R file”. Figure 1.12: Choosing the dataset to import in SPSS Then click Open (wait a couple of seconds) and click on next. You will see the following window that is part of the Text Import Wizard procedure in SPSS (Figure 1.13): Figure 1.13: Step 1 of the Text Import Wizard Then click the “Next &gt;” button 5 times, passing by the following windows: Step 2 of 6 (Figure 1.14): To change how variables are arranged: here delimited To include variable names included at the top of the file: here Yes. To set the decimal symbol: here a comma. Figure 1.14: Step 2 of the Text Import Wizard Step 3 of 6 (Figure 1.15): On which line number begins the first case: here 2 How cases are represented: Each line is a case. How many cases you want to import: here all cases. Figure 1.15: Step 3 of the Text Import Wizard Step 4 of 6 (Figure 1.16): The delimiters that appear between variables; here the Semicolon. The text qualifier: here Double quote. Remove trailing spaces from string values: skip. Figure 1.16: Step 4 of the Text Import Wizard Step 5 of 6 (Figure 1.17): Here you overwrite the Data format of the variable (you can also change that in the Variable View window, when the data has been read in). Figure 1.17: Step 5 of the Text Import Wizard Step 6 of 6: To save your specifications of the previous steps into a separate file (Figure 1.18). Figure 1.18: Step 6 of the Text Import Wizard Then click finish and the is imported in a new SPSS file. In that file you can of course change all kind of variable and data settings in the Variable View Window. You can also skip step 2 to 5 by clicking the Finish button twice when you are at step 1. Than you use all default settings, which is most of the times a good option. 1.2.13 Installing R Packages When R is installed on your computer also a folder called library is created. This folder contains packages that are part of the basic installation. A package is a collection of different functions written in the R language. Besides packages that are part of the basic installation of R there are also packages that are not part of the basic installation but are written by others, i.e. the add-on packages. Packages can be downloaded from the CRAN website (https://cran.r-project.org/). Currently, there are thousands of user-written packages available on the CRAN website. Before you can use a specific package that is not part of the basic installation, you have to install it in your R library. In this manual we will use the mice package to do all kind of imputation procedures, such as multiple imputation. mice is not part of the R basic installation and you have to install it first. There are several procedures in RStudio to install a package. One way is to use the install.packages function in the Console window: install.packages(&quot;mice&quot;) The mice package will be automatically downloaded from the CRAN website. Another way is to use the window on the right site below and go to the Packages tab. When you click “Install” a new window is opened. Than you can type “mice” on the blank line under “Packages (separate multiple with space or comma):” (Figure 1.19 and Figure 1.20). Figure 1.19: Install packages Window in RStudio to install packages from the CRAN website Figure 1.20: Enlarged Install packages Window in RStudio to install packages from the CRAN website After you have clicked on “Install” the package will be downloaded from the CRAN website automatically and will be listed in the Package list named “User Library”. Another way is to go to the CRAN website and download the package as a zip file in a directory on your computer, for example your working directory or in your library. Again use the window on the right site below and go to the Packages tab. When you choose Install a new window is opened. Now under “Install from:” choose for “Package Archive File (.zip; .tar.gz)” (Figure 1.21 and Figure 1.22). Than you can browse to the zip file and install the package. Figure 1.21: Install packages Window in RStudio to install packages from zip files Figure 1.22: Enlarged Install packages Window in RStudio to install packages from zip files 1.2.14 Loading R Packages Once an add-on (user written) R package has been installed you have to load it to get access to all functions that are part of that package. To load a library, you can use the function library() or require(). You have to load add-on packages each time you start a new R session. 1.2.15 Updating R Packages To keep the add-on packages up to date you can use the update.packages() function. update.packages() R will ask you if you want to update each package. If you type “y” in the Console window, R will update the package. In RStudio updating packages can be done in the Package tab as well. You can click on the Update button. A new window will open that contains a list of all packages that need to be updated. Subsequently you can select the packages you want to update. 1.2.16 Useful Missing data Packages and links The main package that we will use in this manual is mice which stand for Multivariate Imputation by Chained Equations (MICE) (Van Buuren, 2009). Other packages that can be used to impute data or that can be used to do analyses after imputation are listed below. miceadds Package contains some additional multiple imputation functions (Robitzsch et al., 2017). See for more information: linked phrase micemd Package contains additional functions for the mice package to perform multiple imputation in two-level (Multilevel) data (Audigier &amp; Resche-Rigon, 2017). See for more information: linked phrase mi Provides functions for data manipulation, imputing missing values in an approximate Bayesian framework, diagnostics of the models used to generate the imputations, confidence-building mechanisms to validate some of the assumptions of the imputation algorithm, and functions to analyze multiply imputed data sets (Gelman et al., 2015). See for more information: linked phrase MItools Small package to perform analyses and combine results from multiple-imputation datasets (Lumley, 2015). See for more information: linked phrase norm Package is for the Analysis of multivariate normal datasets with missing values. It contains the mi.inference function. This function combines estimates and standard errors to produce a single inference. Uses the technique described by Rubin (1987), which are called the Rubin’s Rules (RR) (Novo, 2015). See for more information: linked phrase vim (visualization and imputation of missing values) Package includes tools for the visualization of missing and/or imputed values. In addition, the quality of imputation can be visually explored using various univariate, bivariate, multiple and multivariate plot methods (Templ et al., 2017). See for more information: linked phrase BaylorEdPsych Package for Baylor University Educational Psychology Quantitative Courses. This package included Little’s MCAR test (Beaujean, 2015). See for more information: linked phrase MKmisc Contains several functions for statistical data analysis; e.g. for sample size and power calculations, computation of confidence intervals, and generation of similarity matrices. This package contains the mi.t.test function for pooling t-tests after multiple imputation (Kohl, 2016). See for more information: linked phrase mvnmle Package estimates the maximum likelihood estimate of the mean vector and variance-covariance matrix for multivariate normal data with missing values. This package is needed for the mlest function this is used for Little’s MCAR test in Cahpter 2. See for more information: linked phrase "],
["missing-data-evaluation.html", "Chapter 2 Missing Data Evaluation 2.1 Definition of Missing Data 2.2 Missing data Patterns 2.3 2.3 Missing data Mechanisms 2.4 Missing Data evaluation", " Chapter 2 Missing Data Evaluation Before you decide what to do with your missing data it is important to consider the reasons and probable causes of your missing data problem. With that information you can compose an analysis plan to deal with the missing data in your dataset. In this Chapter, you will learn how to explore and evaluate your missing data in SPSS and R and why it is important to think about the missing data mechanism. This knowledge is important for the method you choose to handle the missing data problem. 2.1 Definition of Missing Data 2.1.1 Defining Missing Data in SPSS Missing data in SPSS can be defined in two ways, as a system missing or user missing value. System missing data is missing data that is not present in the dataset and can be recognized by an empty cell (or dot). User missing data is data that is coded as missing value in the dataset by the user for some specific kind of reason. As an example we use a small dataset with 50 Backpain patients consisting of males (coded as 1) and females (coded as 0) patients (Figure 2.1). The female patients in this dataset have been pregnant and the Gestational Age (GA) variable, contains the duration of their pregnancy in weeks. Figure 2.1: SPSS dataset containing variables with system and user missing data The Variable GA in the dataset consists of different values, like real values for GA as 36 and 29, the value 8 and empty cells. The value 8 is specified by us to exclude males from further analysis that include the GA variable. This is a user missing value, that was indicated because males cannot be pregnant. The system missing values are recognizable by the empty cells (or dots) in the dataset, and these indicate the missing GA values for women who did not report their GA. It makes no difference if we code the missing values as a system or user missing value in SPSS, because both kinds of missing values are recognized as missing values by SPSS and will be excluded from further analyses. 2.1.2 Missing data in R In R the missing values are denoted by NA which means “Not Available”. If we open the same dataset as above in R we get the following result. library(haven) dataset &lt;- read_sav(&quot;data/CH2 example.sav&quot;) head(dataset,10) ## # A tibble: 10 x 7 ## ID Pain Tampascale Disability Radiation Gender GA ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 9 45 20 1 1 8 ## 2 2 6 43 10 0 1 36 ## 3 3 1 36 1 0 1 8 ## 4 4 5 38 14 0 0 NA ## 5 5 6 44 14 1 1 8 ## 6 6 7 43 11 1 0 29 ## 7 7 8 43 18 0 0 NA ## 8 8 6 43 11 1 0 34 ## 9 9 2 37 11 1 1 8 ## 10 10 4 36 3 0 0 38 The Variable Gestational Age (GA) contains the values for GA (e.g. 36, 29, etc.), the value 8 for males and the NA’s. In R the value 8 will be treated as a real value, so we have to recode that value to NA by using the following code. dataset$GA[dataset$GA==8] &lt;- NA head(dataset,10) ## # A tibble: 10 x 7 ## ID Pain Tampascale Disability Radiation Gender GA ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 9 45 20 1 1 NA ## 2 2 6 43 10 0 1 36 ## 3 3 1 36 1 0 1 NA ## 4 4 5 38 14 0 0 NA ## 5 5 6 44 14 1 1 NA ## 6 6 7 43 11 1 0 29 ## 7 7 8 43 18 0 0 NA ## 8 8 6 43 11 1 0 34 ## 9 9 2 37 11 1 1 NA ## 10 10 4 36 3 0 0 38 The NA values will be recognized as missing values. For most functions in R the handling of NA values has to be defined. For example, the following code to obtain the mean of Gestational Age results in an NA because the handling of missing data is not defined. mean(dataset$GA) ## [1] NA To obtain the mean of the observed data the following code has to be used: mean(dataset$GA, na.rm=TRUE) ## [1] 35.09524 The na.rm=TRUE statement in the mean-function, indicates that values that are NA need to be removed before the analysis can be executed. Another NA handling procedure that is used in functions is called na.action with as options na.fail, na.omit, NULL (no action) and na.exclude. For more information about na.action options you can type the following code in the R console ?na.action. 2.2 Missing data Patterns To get an idea about the complexity of the missing data problem in your dataset and information about the location of the missing values, the missing data pattern can be evaluated. Historically, the missing data pattern was important as a starting point to choose the missing data handling method (Little and Rubin, 2002). Currently, the missing data pattern is less important because the most advanced (missing) data analysis methods as multiple imputation can handle almost any missing data pattern. We will discuss some frequently seen missing data patterns, which are graphically displayed in Figure 2.2. Figure 2.2: Missing data patterns (ID means person identification number, X1 to X3 represent variables, Time 1 to 3 means that data is measured at 3 time points over time, Study means study number). The white cells represent the missing data A univariate missing data pattern is a pattern with missing values in only one variable. An example of such a pattern is when the independent variables are completely observed, but the outcome variable is not, or when a selection of subjects refuse to fill in a specific question such as their income level. The second and third pattern are examples of multivariate missing data patterns, where multiple variables contain missing values. The second pattern is an example where subjects miss values of the same two variables and the third pattern a more general pattern where different subject miss different variable scores. A monotone pattern of missing data may occur in a longitudinal study with data repeatedly assessed over time, and subjects “drop-out” of the study (fourth pattern). An example could be an elderly study where persons become too frail to participate or just because persons do not want to attend the study anymore because they are not interested to fill in several questionnaires. A pattern called File Matching can be observed when data from several studies is merged for an individual participant data analysis and variables are not assessed in all studies. In our example, one variable is observed in both studies (X1), but X2 and is only observed in study 1 and X3 in study 2. 2.2.1 Missing data patterns in SPSS To evaluate the missing data pattern, we can make use of the options under the Missing Value Analysis (MVA) procedure in SPSS (IBM, 2016). We use as an example a dataset that contains information of 150 Back pain patients and 9 study variables. The variables are Pain (continuous), Tampa scale (continuous), Radiation in the leg (dichotomous), Disability (continuous), Body Weight (continuous), Body Length (continuous), Age (continuous), Smoking (dichotomous), Gender (dichotomous). Only the variables Gender and Age are completely observed. To access the MVA function in the SPSS menu choose: &gt;Analyze -&gt; Missing Value Analysis… A new window will open that is called “Missing Value Analysis” (Figure 2.3) Figure 2.3: The Missing Value Analysis menu From this menu we first transfer all variables of interest in the correct Quantitative and Categorical variables window and then choose for the Patterns option. From the Patterns menu choose for the options Tabulated cases, grouped by missing value patterns and sort variables by missing value pattern. To obtain the full list of all patterns that occur in the data, set the “Omit patterns with less than 1% of cases at 0%, then click on continue and OK. This will produce the output table that is displayed in Figure 2.6. Figure 2.4: The Patterns menu From this menu we first transfer all variables of interest in the correct Quantitative and Categorical variables window and then choose for the Patterns option. From the Patterns menu choose for the options “Tabulated cases, grouped by missing value patterns” and “sort variables by missing value pattern”. To obtain the full list of all patterns that occur in the data, set the “Omit patterns with less than 1% of cases at 0%, then click on continue and OK. This will produce the output table that is displayed in Tables 2.1a and 2.1b. Figure 2.5: The Patterns menu As default procedure univariate statistics are presented including output information about the number and percentages of missing data and other descriptive statistics for each variable. Information about the missing data patterns is provided in the Tabulated patterns table. On the left column of that table, named “Number of Cases”, the number of cases are presented with that specific missing data pattern. In our example, there are 75 cases in total without any missing values and 13 cases with a missing value in only the Tampa scale variable (see row 1 and 2 of Table 2.1). In the right column of that table named “Complete if…”, the total number of subjects is presented if the variables that contain missing data in that pattern are not used in the analysis. Those variables are marked with the “X” symbol. For example, 88 subjects will be included in the analysis when the variable Tampa scale is not used in the analysis, those are the 75 subjects that have completely observed data on top of the 13 subjects with missing data in the Tampa scale variable only. Figure 2.6: Descriptive missing data statistics and the missing data patterns. Figure 2.6: Descriptive missing data statistics and the missing data patterns. Another way to obtain information about the missing data patterns is via the Multiple Imputation option. To access this option, choose: &gt; Analyze -&gt; Multiple Imputation -&gt; Analyze Patterns… Figure 2.7: Analyse Patterns menu. Now transfer all variables that have to be analyzed for their missing values to the window “Analyze Across Variables”. We choose for the following output options in that window: Summary of missing values (displays missing data information in pie charts, Patterns of missing values (displays tabulated patterns of missing values) and Variables with the highest frequency of missing values (displays a table of analysis variables sorted by percent of missing values in decreasing order). To obtain the full list of all patterns set the “Minimum percentage missing for variable to be displayed” at 0. You can also adjust the maximum number of variables displayed. This procedure will generate the following output: Figure 2.8: Output as a result of the Analyze Patterns menu under Multiple Imputation. Figure 2.8: Output as a result of the Analyze Patterns menu under Multiple Imputation. Figure 2.8: Output as a result of the Analyze Patterns menu under Multiple Imputation. Figure 2.8: Output as a result of the Analyze Patterns menu under Multiple Imputation. Summary of missing values (displays missing data information in pie charts, Patterns of missing values (displays tabulated patterns of missing values) and Variables with the highest frequency of missing values (displays a table of analysis variables sorted by percent of missing values in decreasing order). To get the full list of all patterns set the “Minimum percentage missing for variable to be displayed” at 0. You can also adjust the maximum number of variables displayed. This procedure will generate the following output. 2.2.2 Missing data patterns in R To generate the missing data patterns in R we can make use of the mice and VIM packages. We start with the mice package. That package contains the md.pattern function that can generate the missing data pattern. library(mice) ## Loading required package: lattice ## ## Attaching package: &#39;mice&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## cbind, rbind md.pattern(dataset) ## ID Pain Tampascale Disability Radiation Gender GA ## 21 1 1 1 1 1 1 1 0 ## 29 1 1 1 1 1 1 0 1 ## 0 0 0 0 0 0 29 29 The first row contains the variable names. Each other row represents a missing data pattern. The 1’s in each row indicate that the variable is complete and the 0’s indicate that the variable in that pattern contains missing values. The first column on the left (without a column name) shows the number of cases with a specific pattern and the column on the right shows the number of variables that is incomplete in that pattern. The last row shows the total number of missing values for each variable. To obtain a visual impression of the missing data patterns in R we use the VIM package. That package contains the function aggr As a result of using this function, the univariate proportion of missing data is given in the Console window together with two graphs. library(VIM) ## Loading required package: colorspace ## Loading required package: grid ## Loading required package: data.table ## VIM is ready to use. ## Since version 4.0.0 the GUI is in its own package VIMGUI. ## ## Please use the package to use the new (and old) GUI. ## Suggestions and bug-reports can be submitted at: https://github.com/alexkowa/VIM/issues ## ## Attaching package: &#39;VIM&#39; ## The following object is masked from &#39;package:datasets&#39;: ## ## sleep aggr(dataset, col=c(&#39;white&#39;,&#39;red&#39;), numbers=TRUE, sortVars=TRUE, cex.axis=.7, gap=3, ylab=c(&quot;Percentage of missing data&quot;,&quot;Missing Data Pattern&quot;)) ## ## Variables sorted by number of missings: ## Variable Count ## GA 0.58 ## ID 0.00 ## Pain 0.00 ## Tampascale 0.00 ## Disability 0.00 ## Radiation 0.00 ## Gender 0.00 A histogram is displayed with the univariate percentage of missing values in each variable, which is also shown as output in the Console window and the patterns of missing data are also displayed. At the right side of Figure 2.8 the proportion of patterns is presented. The variable names are shown at the bottom of the figures. The red cells in the Missing data patterns figure indicate that those variables contain missing values. We see that 0.500 or 50% of the patterns do not contain missing values in any of the variables. Of the total patterns, 8.67% of the patterns have missing values in only the Tampa scale variable. 2.3 2.3 Missing data Mechanisms By evaluating the missing data patterns, we can get insight in the location of the missing data. With respect to the missing data mechanism we are interested in the underlying reasons for the missing values and the relationships between variables with and without missing data. In general, we can say that missing values are either random or non-random. Random missing values may occur because subjects accidentally do not answer some questions or information of an entire subject is accidentally not assessed. For example, a study subject has to fill out some questionnaire instruments, gets distracted and misses a question accidently or a questionnaire gets lost in the mail. Non-random missing values may occur because subjects purposefully do not answer questions. For example, subjects may be reluctant to answer questions about sensitive topics like income, past crimes or sexual history. Rubin introduced in 1976 a typology for missing data that makes a distinction between random and non-random missing data situations, which are abbreviated as MCAR, MAR and MNAR. These types of missing data are still used as the basic missing data mechanisms. The key idea behind Rubin’s missing data mechanisms is that the probability of missing data in a variable may or may not be related to the values of other measured variables in the dataset. This means that we assume that there is some kind of probability model for the missing data. With probability we loosely mean the likelihood of a missing value to occur, i.e. if a variable has a lot of missing data, the probability of missing data in that variable is high. This probability (i.e. likelihood) can be related to other measured or not-measured variables. For example, when mostly older people have missing values, the probability for missing data is related to age. Moreover, the missing data mechanisms also assume a certain relationship (or correlation) between observed and variables with missing values in the dataset. The extend of the relation between observed variables and the probability of missing data, distinguishes the three missing data mechanisms. In essence the missing data mechanisms describe relationships between variables that may or may not be causal, because in most missing data situations we never know the real reason why data is missing. We will discuss the missing data mechanisms in more detail below. As an example, we will use a study on Low Back Pain (LBP). It is known that people with LBP may develop a fear of movement (which is assessed by the Tampa scale) due to their pain in the back. The idea is that these people believe that some underlying serious problem causes their back pain and in order to prevent for more damage they are afraid to move their back and experience a high fear of movement. 2.3.1 2.3.1 Missing Completely At Random Data are Missing Completely At Random (MCAR) when the probability that a value is missing, is unrelated to the value of other observed (or unobserved) variables, and unrelated to values of the missing data variable itself. We will discuss what is means by using the LBP study as an example. If LBP patients had to come to a research center to fill in the Tampa scale (fear of movement) questionnaire and supply other information for the study and some patients were not able to come because they were ill that day due to the flu. In case of MCAR, there is no relationship between having the flu and scores on the Tampa scale or other study-related variables. This is realistic because there is no evidence that patients with the flu, fear their back problem more, or that the odds of having the flu is related to the study. For that reason, we can assume that the missing data are MCAR. In other words, the probability of missing data is not related to the values of the Tampa scale variable. Another example is when respondents accidentally skip questions in a questionnaire. Than the observed values of that questionnaire are just a random sample of the entire dataset. An MCAR missing data situation for the Tampa scale variable is visualized in the MCAR column in the figure below. Although in real live we actually do not know the completely observed data, as an example (for educational reasons), the MCAR column is a copy of the completely observed Tampa scale variable, with some values removed. When we compare the MCAR data with the complete Tampa scale variable scores, we can observe that in the MCAR situation an equal number of lower and higher values of the Tampa scale variable are missing (in total 4 Tampa scores are missing, 2 for lower and 2 for higher values,). Also, the missing data in the Tampa scale do not seem to be related to another variable like pain; an equal number of Tampa scale values is missing for patients with low pain scores as well as for patients with higher pain scores. This means that the (observed) probability of missing data in the Tampa scale variable will be equally large for lower and higher values of the Tampa scale and of other measured variables in the data (i.e. pain). Figure 2.9: Examples of MCAR, MAR and MNAR data. This phenomenon for the Tampa scale variable in the LBP study is shown in Figure @(fig:tab2-2) below. This shows that the percentages (or observed probabilities) of missing data are equally large for lower, middle and higher values of the Tampa scale variable. These values are presented as Percentile Groups of Tampa scale values with a range of 28-34, 35-37, 38-40, 41-44, 45-50 respectively. Over the whole range, the percentage of missing data is around 25% (equally large for different values and (not shown) this is also the case for lower and higher values of other variables e.g. pain). In general, The MCAR mechanism does not lead to parameter estimation problems as for example invalid mean or regression coefficient estimates. However, excluding cases with MCAR data will result in a smaller dataset for the analysis and thus larger standard errors (i.e. lower power). Figure 2.10: MCAR missing data in the Tampa scale variable. 2.3.2 Missing At Random The data are Missing At Random (MAR) when the probability that a value for a variable is missing is related to other observed values in the dataset but not to the variable itself. The reason of missing data may lie outside the dataset but observed variables in the dataset capture this by their associations. An example of MAR data is presented in the MAR column of Figure 2.9 and Figure 2.11 for 150 LBP patients. In Figure 2.9 you can see in the MAR column that 4 Tampa scale scores are missing for pain scores that are &gt; 5 and 1 for a pain score &lt; 6, in other words the probability of missing data in the Tampa scale scores is higher for higher pain scores. In other words, patients with higher pain scores (&gt; 5) have more missing values on the Tampa scale variable than patients with lower pain scores (&lt; 6). However, within the category of pain scores with values &gt; 5, the Tampa scale scores are MCAR, because within each category Tampa scale scores are randomly missing for lower and higher values. In 2.11 the Tampa scale data is split for at lower and higher pain score categories. It can be observed that the means and standard deviations do not differ between the observed and missing data for the Tampa scale variable. Which is an indication that the Tampa scale data are MAR. (for educational purposes we know the values of the missing values). A reason could be that patients with higher Tampa scale scores were less likely to show up at a next Tampa scale measurement because their back hurted more. In a MAR missing data situation, missing values can be explained by other (observed) variables, like for the Tampa scale and Pain variable in the example above, due to the their (statistical) relationship in the dataset. Further, within categories of the pain variable (for low and high pain values) the Tampa scale scores are MCAR (Raghunathan, 2016). However, it is not possible to test this assumption, because for that you need information of the missing values and that is impossible. In general, excluding MAR data leads to false estimates of your statistical tests, like for example, regression coefficients. A missing data method that works well with MAR data is Multiple Imputation (Chapter 4). Figure 2.11: MAR missing data in the Tampa scale variable. 2.3.3 Missing Not At Random The data are MNAR when the probability of missing data in a variable is related to the scores of that variable itself, e.g. only high or low scores are missing, and this missing data problem cannot be captured anymore by other measured variables in the dataset because they are not available in the dataset. In case of the LBP example, MNAR data occurs when patients with the highest scores on the Tampa scale have missing values. This is shown in the MNAR column of Figure 2.9. The MNAR column shows that the values that are most frequently missing are the highest Tampa scale scores, i.e. for the patients that fear their back problems most. A reason may be that these patients were so afraid to move (which is assessed by the Tamp scale), that they were not able to visit an assessment center. MNAR missing data can also occur indirectly through the relationship of the variable with missing data with another variable that is not available in the dataset. For example, it could also be that patients that worry the most do not want to be confronted with questions about their fear to move their back and therefore skip questions of the Tampa scale. In case of a positive relationship between worriedness and fear of movement, the highest values on the Tampa scale variable will be missing for those patients that worry the most. If worriedness is not measured in the dataset, the missing data in the Tampa scale variable will be MNAR. The difference with MAR is that with MNAR, the missing data problem cannot be handled by the observed variables in the dataset or by using a technique as Multiple Imputation. However, as with MAR data, MNAR data can also not be verified. 2.3.4 The Missing Data Indicator In the definitions of the missing data mechanisms in the previous paragraphs we used the term probability several times, to indicate that the relationship of variables with the probability of missingness in a variable distinguishes the missing data mechanisms. The probability of missing data can depend on other variables (MAR), on values of the variables itself (MNAR) or not on other variables or values of the variable itself (MCAR). Rubin (1987) proposed that variables with missing data can be divided in a part that is observed and a part that is missing. The observed and missing data can be coded by a 0 and 1 respectively. In case of the Tampa scale variable this means that the observed data is coded by a 0 and a 1 is used for the Tampa scale values that are missing. This dichotomous coding variable is called the missing data indicator or R variable which means that the complete and missing data are defined according to this variable. Figure 2.12 shows the R indicator variable for the observed and missing data in the Tampa scale variable. The R variable is now a single variable because there is missing data in only the Tampa scale variable. When more variables contain missing data, the R variable becomes a dataset of variables that consist of 0´s and 1´s. Figure 2.12: The missing data in the Tampa scale variable coded according to the missing data indicator variable R. Using the R indicator variable implies that missing values (or the probability of missing values) can be described by a missing data model. This missing data model may consist of variables that have a relationship with the probability of missing data, in this case the R indicator variable. A good example would be to use a logistic regression model to describe the relationship of variables with the probability of missing data in the Tampa scale variable. Graphically these models can be visualized as in Figure 2.11. With logistic regression, which is in essence a probability model, the relationship of a dichotomous outcome variable (i.e. the R missing data indicator variable) with other variables can be described by using the models that are shown in Figure 2.12. In this Figure the missing data indicator variable R of the Tampa scale variable that contain missing values is the outcome variable. Figure 2.13: MCAR There is no relationship between, how the data became missing (indicated by R) and observed and unobserved variables. Figure 2.14: MAR There is a relationship between, how the data became missing (indicated by R) and observed variables. Figure 2.15: MNAR There is a relationship between, how the data got missing (indicated by R) and observed and unobserved variables. The missing data mechanisms of Rubin can then be described by the following logistic regression models: MCAR: No variables in or outside the dataset explain the missingness in the Tampa scale variable, the model would be empty: \\[LN(\\frac{R_{Tampa=1}}{1-R_{Tampa=1}}) = \\beta_0\\] MAR: Variables in the dataset, like Pain and Radiation in the Leg explain the missingness in the Tampa scale variable and the model would be: \\[LN(\\frac{R_{Tampa=1}}{1-R_{Tampa=1}}) = \\beta_0 + \\beta_1 * Pain +\\beta_2 * Radiation\\] MNAR: Missingness in the Tampa scale variable can be explained by variables in the dataset and by the score on the Tampa scale itself: \\[LN(\\frac{R_{Tampa=1}}{1-R_{Tampa=1}}) = \\beta_0 + \\beta_1 * Pain + \\beta_2 * Radiation + \\beta_3 * Tampa\\] From these models we observe that the missing data models can be defined by other observed variables in the dataset and/or missing data (which we do not observe). It should be noted however, that we can never be completely sure about the reason why data are missing. The above mentioned relationships are therefore statistical relationships from descriptive models and not from causal models. In summary, the MCAR assumption is mostly very strict and not realistic in practice.The MAR assumption is more realistic and mostly assumed in practice.The difference bewteen MAR and MNAR is that in MNAR the probability of missingness is also related to the unobserved (missing) data. 2.3.5 The Role of Auxiliary Variables Usually, the probability of missing data is related to other variables and/or to the missing values itself (MAR or MNAR). Unfortunately, it is not possible to distinguish between MAR and MNAR mechanisms, because the missing values are unknown. Brand (Brand, 1999) describes in Chapter 2 of his dissertation two examples that demonstrate how an initially MNAR missing data mechanism can change into MAR by including additional variables that are related to the probability of missing data. In practice, by including variables related to the probability of missing data a MNAR mechanism can get closer to a MAR mechanism. Accordingly, the MAR assumption can be made more plausible by including additional information in the missing data handling method (Baraldi &amp; Enders, 2010). Therefore, it is advised, to include extra variables that have a relationship with the missing data rate in other variables, i.e. have a relationship with the probability of missing data or that have a relationship (correlated) with the variables that contain the missing values (Collins, 2001, Curran, Bacchi, Schmitz, Molenberghs, &amp; Sylvester, 1998). These variables are called auxiliary variables and are included in the imputation procedure to generate valid imputations. These variables may not be required for further data analyses. In general, the MCAR assumption is unrealistic and does not hold in most datasets. As a practical solution, many studies assume a MAR mechanism. Mostly, this mechanism is realistic, however it is advised to evaluate the plausibility of this assumption in analyses explained below. 2.4 Missing Data evaluation A useful data evaluation and data imputation method depends on the underlying missing data mechanism that is assumed. We have seen in Paragraph 2.3 that the difference between the MCAR and not MCAR mechanisms depend on the relationship of the missing data with the observed variables. If this relationship cannot be detected by the observed data in the dataset we assume that the data is MCAR. If there is some kind of relationship, the missing data may be MAR or MNAR. We can never distinct between MAR or MNAR data, because for that we need information about the missing values and we do not have that information. Tests to distinguish the missing data assumptions are therefore only aimed to accept or reject the MCAR missing data assumption. The MCAR assumption is mostly a too strict assumption to use in practice because there is commonly some kind of reason why persons do not fill in “working” questions. Moreover, in practice we study and measure outcome and independent variables that are related to each other. This makes that the MAR assumption is the most accepted “working” missing data assumption in practice. There are two ways to evaluate the missing data mechanism. First, it is important to think about the most plausible substantive reasons for the data being missing. Researchers mostly have some possible explanation about why data are missing and this information is very important. For example, when during web based data collection, the internet sometimes disconnects because of malfunction at the address of the internet provider, data of a few participants gets lost. When these malfunctions are coincidental, it can be assumed that the missing data are MCAR. However, when cognitive scores are assessed during this web based data collection and these are mostly not filled out by people that have decreased cognitive functions, the missing data can be assumed to be MNAR. There are statistical test procedures that can be used to get an idea about the missing data mechanism. In these statistical tests, the non-responders (i.e., participants with missing observations), can be compared to the responders on several characteristics. By doing this, we can test whether the missing data mechanism is likely to be MCAR or not-MCAR (because we cannot distinguish between MAR and MNAR missing data). There are several possibilities to compare the non-responders with the responders groups, for example using t-tests, a logistic regression with a missing data indicator as the outcome, or Little’s MCAR test (Little, 1988). To discuss these tests, we will use the example data from the previous chapter, where we have measured several variables in a group of 150 low back pain patients and some variables contain missing observations. In the examples below we will assume a MAR missing data mechanism for the variables Tampa scale and Disability. Researchers need to be aware that the assumptions that underlie an independent t-test, logistic regression, and Chi-square test apply to these missing data mechanism procedures as well. This means that the data is assumed to be normally distributed and that the tests depend on a decent sample size. 2.4.1 Missing data Evaluation in SPSS 2.4.1.1 Descriptive Statistics Descriptive information of variables can be obtained via the following options of the Missing Value Analysis (MVA) module in the SPSS menu: Analyze -&gt; Missing Value Analysis… Transfer all variables in the correct Quantitative and Categorical variables window and then click Descriptives option -&gt; Univariate statistics. Figure 2.16: Missing value Analysis menu in SPSS Figure 2.17: Univariate descriptive statistics of variables with and without missing data. Under the column N, the information of all cases in the dataset are displayed, under the column Missing we get the number and percentage of missing values in each variable and under the column No. of Extremes we get information of cases that fall outside a range, which is specified under the table. Further, for all continuous variables information about the Mean and Standard deviation are displayed. No descriptive information is given for categorical variables. These descriptive information of variables with missing data gives us a quick overview of the amount of missing data in each variable. However, it does not provide us information about the relationship between variables with complete and missing data and therefore does not give us an idea about the potential missing data mechanism. Methods as T-tests, regression or Little’s MCAR test, that will be discussed in the next section, can better be used for that purpose. 2.4.1.2 T-test procedure When we use the t-test procedure, SPSS first creates an indicator variable, to distinguish the cases with missing values from the cases with values that are present in each variable with missing values. Then, group means are estimated and compared for other variables that are complete with groups formed by the indicator variable, using Student’s t test. As a result, the t-statistic, degrees of freedom, counts of missing and non-missing values, and means of the two groups are displayed. You can also display any two-tailed probabilities associated with the t statistic. The t-test procedure can be found in: &gt;Analyze -&gt; Missing Value Analysis… -&gt; Descriptives -&gt; click “t-tests with groups formed by indicator variables” and “include probabilities in table” -&gt; Continue -&gt; OK. Figure 2.18: The T-test procedure as part of the Missing Value Analysis menu Figure 2.19: Output table of the t-test procedure. On the left side oft the output table the names of the variables with missing values are presented which are the Tampa scale and Disability variables. Of these variables, indicator variables are defined which are used to compare group means of other variables, that can be tested for significance using independent t-tests. The results of these t-tests are given in the table according to the information in separate rows on the left side with the t-value (t), degrees of freedom (df), P-value (P(2-tail)), numbers of observed and missing cases (Present and Missing) and means of observed and missing cases (Mean(Present) and Mean(Missing)) presented. The variables for which the indicator groups are compared, are listed in the columns of the table and are the Pain, Tampa scale, Disability and Age variables. For the Tampa scale variable that contain missing values, only the observed mean is presented, because for the missing cases the values are missing! Notice that in the row of the Tampa scale variable the means of the Disability variable can still be compared between the observed and missing cases, because they do not miss values for exactly the same cases. Table 2.5 shows that patients that have observed values on the Tampa scale variable (row Mean(Present)) differ significantly from patients with missing values on the Tampa scale variable (row Mean(Missing)) on Pain (P(2-tail = 0.033) and Disability (P(2-tail = 0.039). When we look at the means of the Pain variable, we see that the mean of patients with missing values on the Tampa scale variable is higher compared to the mean of patients with observed scores. This means that there is a higher probability of missing data on the Tampa scale variable for patients with higher pain scores. If Tampa scale and Pain scores are correlated, the missing values on the Tampa scale variable can also be explained by the Pain score variable. This is also the case for the Age variable, but now the t-test is not significant. For the Disability variable, it is the other way around. We see more missing data on the Tampa scale variable for lower Disability scores. In the Missing Value Analysis window under the Indicator Variable Statistics options you can also choose for “Crosstabulations of categorical and indicator variables”. In that case a separate table is displayed for each categorical variable with missing values. For each category of the variable, the frequency and percentages of non-missing values for the other variables is displayed as well as the percentages of each type of missing value. To reduce the size of the table, you can omit statistics that are computed for only a small number of cases by adjusting the option “Omit variables missing less than % of cases”. 2.4.1.3 Logistic Regression Analysis The missing data mechanism can also be evaluated with a logistic regression procedure (Ridout, Society, &amp; Diggle, 1991). In the logistic regression analysis, we can evaluate if the probability of missing data is related to other variables in the data. For this procedure, we first generate an indicator variable that separates the subjects with missing values from the participants with observed values. This indicator variable is used as the dependent variable in a logistic regression analysis. A backward regression can be used to determine the strongest predictors of missing data. The output for the logistic regression with the Tampa scale variable as the indicator outcome variable is presented below: Figure 2.20: Logistic regression analysis with variable that contain missing data as the outcome variable. We can observe that the variable Pain is significantly related to the missing data indicator variable of the Tampa scale variable, which indicates that the probability for missing data in the Tampa scale variable can be explained by the Pain variable. The positive coefficient of 0.315 indicates that the probability of missing data on the Tampa scale variable is higher for higher Pain scores. The other variables do not show a significant relationship with missing data on the Tampa scale variable. This logistic regression analysis procedure can be repeated for each variable with missing values in the dataset. 2.4.1.4 Little’s MCAR test in SPSS Another possibility is to use a test that was developed by Roderick Little: Little’s MCAR test. This test is based on differences between the observed and estimated mean in each missing data pattern. This test is developed for continuous data. In this procedure, the missing data in the whole dataset is evaluated, because each missing data pattern is included in the analysis. To conduct this test, you choose from the SPSS menu: Analyze -&gt; Missing Value Analysis…In the main Missing Value Analysis dialog box, select the variable(s) for which you want to estimate missing values (use only continuous variables) and Select EM in the Estimation group and Click OK. Figure 2.21: EM selection in the Missing Value Analysis menu. Figure 2.22: Output tables with information of Little’s MCAR test. Figure 2.22: Output tables with information of Little’s MCAR test. Figure 2.22: Output tables with information of Little’s MCAR test. Figure 2.22: Output tables with information of Little’s MCAR test. Figure 2.22: Output tables with information of Little’s MCAR test. 2.4.2 Missing data Evaluation in R 2.4.2.1 Little’s MCAR test in R Little´s MCAR test is available in the BaylorEdPsych package for R as the LittleMCAR function. To apply the test, we select only the continuous variables. In the example blowe we use the dataset of 150 low back pain patients with missing data in Genstatstional Age (GA). The p-value for the test is not-siginificant, indicating that the missings seem to be compeletely at random. library(BaylorEdPsych) LittleMCAR(dataset[,c(&quot;Pain&quot;, &quot;Tampascale&quot;,&quot;Disability&quot;, &quot;GA&quot;)]) ## Loading required package: mvnmle ## this could take a while ## $chi.square ## [1] 5.395246 ## ## $df ## [1] 3 ## ## $p.value ## [1] 0.14504 ## ## $missing.patterns ## [1] 2 ## ## $amount.missing ## Pain Tampascale Disability GA ## Number Missing 0 0 0 29.00 ## Percent Missing 0 0 0 0.58 ## ## $data ## $data$DataSet1 ## Pain Tampascale Disability GA ## 2 6 43 10 36 ## 6 7 43 11 29 ## 8 6 43 11 34 ## 10 4 36 3 38 ## 13 0 32 3 42 ## 15 3 34 13 39 ## 17 3 35 11 26 ## 20 4 32 9 28 ## 25 5 36 6 35 ## 28 3 36 3 36 ## 30 6 37 16 40 ## 32 4 37 8 39 ## 34 2 37 3 37 ## 37 8 47 8 35 ## 39 3 39 8 33 ## 41 7 45 10 32 ## 44 1 35 2 34 ## 46 5 41 17 38 ## 47 6 43 11 41 ## 48 3 39 9 33 ## 50 8 44 19 32 ## ## $data$DataSet2 ## Pain Tampascale Disability GA ## 1 9 45 20 NA ## 3 1 36 1 NA ## 4 5 38 14 NA ## 5 6 44 14 NA ## 7 8 43 18 NA ## 9 2 37 11 NA ## 11 5 38 16 NA ## 12 9 47 14 NA ## 14 6 38 12 NA ## 16 6 42 8 NA ## 18 1 31 1 NA ## 19 2 31 7 NA ## 21 5 39 13 NA ## 22 5 39 12 NA ## 23 4 34 8 NA ## 24 8 47 13 NA ## 26 5 38 16 NA ## 27 9 48 23 NA ## 29 2 36 9 NA ## 31 10 43 21 NA ## 33 10 42 20 NA ## 35 6 43 12 NA ## 36 3 38 7 NA ## 38 3 38 6 NA ## 40 7 44 15 NA ## 42 6 40 12 NA ## 43 7 40 16 NA ## 45 9 41 19 NA ## 49 2 33 6 NA "],
["single-missing-data-imputations.html", "Chapter 3 Single Missing data imputations 3.1 Complete cases analysis 3.2 Mean Imputation 3.3 Regression imputation 3.4 Bayesian Stochastic regression imputation 3.5 Predictive Mean Matching or Regression imputation", " Chapter 3 Single Missing data imputations 3.1 Complete cases analysis Complete case analysis (CCA) means that the statistical analyses is done in the dataset afetr every persons with a missing data point in a variable is excluded from the analysis. This procedure is still used a lot (Eekhout et al. (2012)) but can have a large negative impact on the precision of the statistical test results. It also leads to an incorrect estimation of standard errors when the data is MCAR, MAR and MNAR (Eekhout et al. (2014)). It is for that reason not recommended. It is however, the default procedure in a lot of statistical software packages as in SPSS. 3.2 Mean Imputation Assume that we are interested in the relationship between Pain and the Tampa scale variable. To get a first impression about this relationship we make a scatterplot. The scatterplots of the complete and incomplete datasets are displayed in Figure 3.1 and Figure 3.2. Figure 3.1: Relationship between the Tampa scale and Pain variables (green dots are observed and red dots are assumed to be missing data Figure 3.2: Relationship between the Tampa scale and Pain variable. Missing data are excluded The green dots represent the observed data and the red dots the missing data points. In practice, in your dataset you have the available points that are visualized in Figure 3.2. 3.2.1 Mean imputation in SPSS The easiest methods to do mean imputation are by using the Replace Missing Values procedure under Transform and by using the Linear Regression procedure. Replace Missing Values procedure You can find the Replace Missing Values dialog box via Transform -&gt; Replace Missing Values. A new window opens. Transport the Tampa scale variable to the New variable(s) window (Figure 3.3). The default imputation procedure is Mean imputation or called “Series mean”. Figure 3.3: Window for mean imputation of the Tampa scale variable. When you click on OK, a new variable is created in the dataset using the existing variable name followed by an underscore and a sequential number. The result is shown in Figure 3.4. Figure 3.4: Mean imputation of the Tampa scale variable with the Replace Missing Values procedure. The scatterplot between the Pain and the Tampa scale variable clearly shows the result of the mean imputation procedure, all imputed values are located at the mean value (Figure 3.5). Figure 3.5: Scatterplot between the Tampa scale and Pain variable, after the missing values of the Tampa scale variable have been replaced by the mean. Linear Regression Mean imputation in SPSS is also integrated in the Linear Regression menu via: Analyze -&gt; Regression -&gt; Linear -&gt; Options. In the Missing Values group you choose for Replace with mean (Figure 3.6). Figure 3.6: The option Replace with mean in the Linear Regression menu. You can also obtain the mean values by using Descriptive Statistics, and than replace the missing values by using the “Recode into Same Variables”under the Transform menu. 3.2.2 Mean imputation in R You can do mean imputation by using the mice function in the mice package and choose as method “mean”. library(foreign) # activate the foreign package to use the read.spss function dataset &lt;- read.spss(file=&quot;Backpain 50 missing.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 library(mice) # Activate the mice package to use the mice function imp_mean &lt;- mice(dataset, method=&quot;mean&quot;, m=1, maxit=1) ## ## iter imp variable ## 1 1 Tampascale You can extract the mean imputed dataset by using the complete function. complete(imp_mean) 3.3 Regression imputation 3.3.1 Regression imputation in SPSS You can apply regression imputation in SPSS via the Missing Value Analysis procedures. There are two options for regression imputation, the Regression option and the Expectation Maximization (EM) option. The Regression option in SPSS has some flaws in the estimation of the regression parameters (Hippel (2004)). Therefore, we use the EM algorithm. This algorithm is a likelihood-based procedure. This means that the most likely values of the regression coefficients are estimated given the data and subsequently used to impute the missing value. This EM procedure gives the same results as first performing a normal regression analysis in the dataset and subsequently estimate the missing values from the regression equation, after the missing values have been excluded. We will compare the EM and regression procedures below. 3.3.1.1 EM procedure Step 1, Choose: Analyze -&gt; Missing Value Analysis… In the main Missing Value Analysis dialog box, select the variable(s) that you want to use for the regression method and select EM in the Estimation group (Figure 3.7). Figure 3.7: EM Selection in the Missing Value Analysis window. Step 2 click Variables, to specify predicted and predictor variables. Place the Tampascale variable in the window of the Predicted variables and the Pain variable in the Predictor Variables window (Figure 3.8). Figure 3.8: Transfer of the Tampascale and Pain variables to the Predicted and Predictor Variables windows. Step 3 Click on Continue -&gt; EM and Choose for Normal in the Distribution group. Than thick Save completed data and give the dataset a name, for example “ImpTampa_EM” (Figure 3.9). Figure 3.9: Name of dataset to save the EM results in. Step 4 Click Continue -&gt; OK. The new dataset “ImpTampa_EM” will open in a new window in SPSS. In this dataset the imputed data of the Tampascale Variable together with the original data is stored (Figure 3.10, first 15 patients are shown). Adjust the Width and Decimals of the Tampa scale variable in the Variable View window to 4 and 3 respectively to get the same results as in Figure 3.13. Figure 3.10: Result of the EM procedure. Be aware that SPSS uses as default only quantitative variables to impute the missing values with the EM algorithm. If you want to include categorical variables in the imputation model as auxiliary variables, you have to define them as scale variables. 3.3.1.2 Normal Linear Regression We now compare the EM procedure with a two-step linear regression procedure. We first estimate the relationship between Pain and the Tampa scale variable in the dataset with linear regression after we have excluded the cases with missing values in the Tampa scale variable. Subsequently, we use the regression coefficients from this regression model to estimate the imputed values in the Tampa scale variable. Start by estimating the linear regression model with the Tampa scale variable as outcome variable (because we have to predict missing values in this variable) and use the Pain variable as the independent variable. To estimate the linear regression model, choose: Analyze -&gt; Regression -&gt; Linear Transfer the Tampa scale variable to the Dependent variable window and the Pain variable to the “Independent(s) in the Block 1 of 1 group. Then click OK. Figure 3.11: Linear regression analysis with the Tampa scale as the outcome and Pain as the independent variable. The following coefficients will be estimated (Figure 3.12). Figure 3.12: Result of the linear regression analysis. The linear regression model can be described as: \\[Tampascale = 32.005 + 1.410 × Pain\\] Now impute the missing values in the Tampa scale variable and compare them with the EM estimates. You see that the results are the same. Figure 3.13: Predictions of the missing Tampa scale values on basis of the regression model estimated in the dataset after the missing values were excluded. When you make a scatterplot of the imputations from the regression model you see that, as expected, the imputed values lie directly on the regression line (Figure 3.14). Figure 3.14: Relationship between the Tampa scale and the Pain variable. The Tampa scale variable is located on the y-axis. The imputed values of the Tampa scale variable (red dots) are located on the regression line that was used to generate the imputed values. The green dots are the observed data values. 3.3.2 Regression imputation in R You can aplly regression imputation in R with as method “norm.predict” in the mice function. The Pain variable is used to predict the missing values in the Tampa scale variable. library(foreign) dataset &lt;- read.spss(file=&quot;Mean imputation.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 dataset &lt;- dataset[, c(&quot;Pain&quot;, &quot;Tampascale&quot;)] imp.regress &lt;- mice(dataset, method=&quot;norm.predict&quot;, m=1, maxit=1) ## ## iter imp variable ## 1 1 Tampascale imp.regress$imp$Tampascale # Extract the imputed values ## 1 ## 2 40.46554 ## 6 41.87566 ## 9 34.82506 ## 14 40.46554 ## 21 39.05542 ## 25 39.05542 ## 27 44.69590 ## 31 46.10602 ## 35 40.46554 ## 37 43.28578 ## 44 33.41494 ## 49 34.82506 ## 50 43.28578 Expectantly, this gives comparable results as the regression imputation with SPSS above. The method “norm.predict” in the mice package fits a linear regression model in the dataset after the missing data is excluded and generates the imputed values for the Tampa scale variable by using the regression coefficients of the linear regression model. The same regression coefficients are used to predict the missing values in the Tampa scale variable as shown in Figure 3.12. The completed dataset can be extracted by using the complete function in the mice package. 3.3.3 Stochastic regression imputation With Stochastic regression models imputation uncertainty is accounted for by adding extra error variance to the predicted values that are generated by the linear regression model. Stochastic regression can be activated in SPSS via the Missing Value Analysis and the Regression Estimation option. However, the Regression Estimation option generates incorrect regression coefficient estimates (Hippel (2004)) and will therefore not further discussed. 3.3.4 Stochastic regression imputation in R You can apply stochastic regression imputation in R with the mice function. For this you use as method “norm.nob”. dataset &lt;- read.spss(file=&quot;Backpain 50 missing.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 dataset &lt;- dataset[, c(&quot;Pain&quot;, &quot;Tampascale&quot;)] imp_nob &lt;- mice(dataset, method=&quot;norm.nob&quot;, m=1, maxit=1) ## ## iter imp variable ## 1 1 Tampascale The completed dataset can be extracted by using the complete function in the mice package. 3.4 Bayesian Stochastic regression imputation The difference between the non-Bayesian imputation procedure that you applied above and Bayesian imputation is that with the latter method extra variation is added to the imputed values via the regression coefficients. The idea is used that there is not one true (population) regression coefficient but that the regression coefficient itself follows a (probability) distribution. This is in contrast to the frequentist idea, which assume that there is one true population parameter. In this case the regression coefficient (is reflected by the confidence interval). In this book (and most statistial books) the freuentist approach is used For more information about the theory of Bayesian statistics we refer to the books of Box and Tiao (2007), Enders (2010) and Gelman et al. (2014). 3.4.1 Bayesian Stochastic regression imputation in SPSS In SPSS Bayesian Stochastic regression imputation can be performed via the multiple imputation menu. To generate imputations for the Tampa scale variable, we use the Pain variable as the only predictor. Step 1 To start the imputation procedure, Go to Analyze -&gt; Multiple Imputation -&gt; Impute Missing Data Values. In the first window you define which variables are included in the imputation model. Transfer the Tampa scale and Pain variable to the Variables in Model window. Than set the number of imputed datasets to 1 under Imputations and give the dataset where the imputed values are stored under “Create a new dataset” a name. Here we give it the name “ImpStoch_Tampa” (Figure 3.15). Figure 3.15: The Variables window. Step 2 In the Methods window, choose under Imputation Method for customand then Fully conditional specification (MCMC). Set the Maximum iterations number at 50. This specifies the number of iterations as part of the FCS method (Figure 3.16). We further use the default settings. Figure 3.16: The Methods window. Step 3 In the constraints window (Figure 3.17) click on the Scan Data button and further use the default settings. Figure 3.17: Stochastic regression imputation Step 4 In the Output window we only use the default settings. Figure 3.18: The Output window. Step 5 Now click on OK button to start the imputation procedure The output dataset consists of the original data with missing data plus a set of cases with imputed values for each imputation. These imputed datasets are stacked under each other. The file also contains a new variable, Imputation_, which indicates the number of the imputed dataset (0 for original data and more than 0 for the imputed datasets). The variable Imputation_ is added to the dataset and the imputed values are marked yellow. Figure 3.19: Aanpassen Titel. Figure 3.20: Imputed dataset with the imputed values marked yellow. When we make a scatterplot of the Pain and the Tampascale variable (Figure 3.21) we see that there is more variation in the Tampascale variable, or you could say that the variation in the Tampascale variable is “repaired”. Figure 3.21: Scatterplot of the relationship between Tampascale and the Pain variable, including the imputed values for the Tampascale variable (red dots). The full Multiple Imputation procedure will be discussed in more detail in the next Chapter. 3.4.2 Bayesian Stochastic regression imputation in R The package mice also include a Bayesian stochastic regression imputation procedure. You can apply this imputation procedure with the mice function and use as method “norm”. The pain variable is the only predictor variable for the missing values in the Tampa scale variable. dataset &lt;- read.spss(file=&quot;Backpain 50 missing.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 dataset &lt;- dataset[, c(&quot;Pain&quot;, &quot;Tampascale&quot;)] imp_b &lt;- mice(dataset, method=&quot;norm&quot;, m=1, maxit=1) ## ## iter imp variable ## 1 1 Tampascale The completed dataset can be extracted by using the complete function in the mice package. 3.5 Predictive Mean Matching or Regression imputation To impute continuous variables two methods can be used. Linear regression imputation or Predictive Mean Matching (PMM).PMM is an imputation method that predicts values and subsequently selects observed values to be used to replace the missing values. It is the default imputation procedure in the mice package (D. B. Rubin (1987)). Linear regression imputation was explained in (paragraph (3.3.1.2)). 3.5.1 Predictive Mean Matching, how does it work? Predictive Mean Matching takes place in several steps: We take as an example the missing values in the Tampa scale variable. They are defined as NA in the dataset. The Pain variable is used to predict the missing Tampa scale values. ## ID Pain Tampascale ## 1 1 5 40 ## 2 2 6 NA ## 3 3 1 41 ## 4 4 5 42 ## 5 5 6 44 ## 6 6 7 NA ## 7 7 8 43 ## 8 8 6 40 ## 9 9 2 NA ## 10 10 6 38 Step 1: Estimate linear regression model A linear regression model is estimated with the Tampa scale variable as the outcome and the Pain variable as the predictor variable for the missing values. We define this regression coefficient as \\(\\hat{\\beta_{Pain}}\\). Step 2: Determine Bayesion version of regression coefficient A Bayesian regression coefficient for the Pain variable is determined (paragraph (XX)). We define this regression coefficient as \\(\\beta_{Pain}^*\\). Step 3: Predict Missing values Observed Tampa scale valueas are predicted by the Pain regression coefficient \\(\\hat{\\beta_{Pain}}\\) from step 1 and the Pain data, we call these values \\(Tampa_{Obs}\\). Missing Tampa scale valueas are predicted by the regression coefficient \\(\\beta_{Pain}^*\\) from step 2 and the Pain data, we call these \\(Tampa_{Pred}\\). These values are: 43.594, 41.456 and 39.852. Step 4: Find closest donor Find the closest donor for the first missing value by subtracting the first \\(Tampa_{Pred}\\) value of 43.594 from all predicted observed values in the \\(Tampa_{Obs}\\) column. These differences are shown in the column Difference. The smallest differences are 1.574, 1.970 and 2.168 and these belong to the cases with observed Tampa scale values of 40, 41 and 42 respectively. Subsequently, a value is randomly drawn from these observed values and used to impute the first missing Tampa scale value. Other missing values are imputed by following the same procedure. The strength of PMM is that missing data is replaced by data that is observed in the dataset and not replaced by unrealistic values (as negative Tampa scale scores). References "],
["data-analysis-after-multiple-imputation.html", "Chapter 4 Data analysis after Multiple Imputation 4.1 Pooling results after MI in SPSS 4.2 Pooling results after MI in R 4.3 Pooling Correlation coefficients 4.4 Pooling Chi-square tests 4.5 Analysis of Variance (ANOVA) pooling 4.6 Pooling Regression models 4.7 Pooling logistic regression models including categorical independent variables", " Chapter 4 Data analysis after Multiple Imputation 4.1 Pooling results after MI in SPSS 4.1.1 Imputed values are yellow After multiple imputation, the multiple imputed datasets are stored in a new SPSS file. In order to obtain pooled analysis results, the imputed values must be marked yellow. Than SPSS recognizes the dataset is an “imputed” dataset and is able to generate pooled analyses results. Figure 4.1) shows an example of a multiple imputed dataset with imputed values marked yellow. If SPSS does not recognize the dataset as a multiple imputed dataset, the data will be treated as one large dataset. Figure 4.1: Example of SPSS dataset after MI has been applied. You can mark the imputed values by using the option “Mark Imputed Data” under the View menu in the Data View window ((Figure 4.1)). 4.1.2 The Imputation_ variable The Imputation_ variable is a nominal variable that separates the original from the imputed datasets. It is used as a variable that splits the file into separate groups for analysis based on the different categories. This is also indicated in the corner on the right side below in the Data View and Variable View windows by the note “Split by imputation_”. Figure 4.2: Procedure to mark imputed values in SPSS. When missing values are imputed with any another software program, you can include an Imputation_ variable in SPSS. It is than recognized as an imputed dataset in SPSS. 4.1.3 Special pooling icon When imputation markings are turned on, a special icon is displayed next to the statistical procedures in the analyze menu. This icon shows you if a pooled result is generated after multiple imputation is used ((Figure 4.3)). Figure 4.3: Multiple Imputation icon. This icon is shown in the analyze menu in SPSS (Figure 4.4)). Figure 4.4: The dataset is recognized as an imputed dataset (special icon visible). SPSS provides two levels of pooling, which are called the Naïve and Univariate combination. The Naïve combination only shows the pooled parameter (if available). The Univariate combination shows the pooled parameter, its standard error, test statistic, effective degrees of freedom, p-value, confidence interval, and pooling diagnostics (fraction of missing information, relative efficiency, relative increase in variance), when available. Although the special icon in SPSS to indicate that the dataset is recognized as a multiple imputed dataset appears for many analysis procedures, it is not always clear what procedures really provide the Univariate combination output. It is therefore recommended to explore what kind of pooled information is provided by SPSS before MI is applied. 4.2 Pooling results after MI in R Many pooling procedures are available as part of the mice package. However, for some specific statistical procedures, other packages are required to obtain pooled estimates. For example, pooling ANOVA results is not available in the mice package itself. For this, the miceadds package has to be used. For the examples in this Chapter We will use three imputed datasets, to keep the output Tables readable, although the examples easily generalize to a larger number of imputed datasets. 4.2.1 Pooling Means and Standard deviations in SPSS To get pooled means you just use Analyze &gt; Descriptive Statistics. Figure 4.5 shows that in the “Pooled” row the mean values of the Tampascale variable are pooled. The standard deviations are not automatically pooled in SPSS. The mean value of the standard deviations can be calculated by computing the average over the standard deviations. Figure 4.5: Pooling results of descriptive statistics. 4.2.2 Pooling Means and Standard Deviations in R To pool the means and standard deviations you use the with function in mice. # Read in the SPSS dataset library(foreign) dataset &lt;- read.spss(file=&quot;Backpain 150 Missing MI datasets.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 library(mice) # Apply multiple imputation imp &lt;- mice(dataset, m=3, maxit=50, seed=2375, printFlag = F) # Stack imputed datasets in long format, exclude the original data impdat &lt;- complete(imp,action=&quot;long&quot;,include = FALSE) # compute mean and standard deviation in each imputed dataset desc &lt;- with(impdat, by(impdat, .imp, function(x) c(mean(x$Tampascale),sd(x$Tampascale)))) desc ## .imp: 1 ## [1] 38.935000 5.330867 ## -------------------------------------------------------- ## .imp: 2 ## [1] 38.988333 5.400321 ## -------------------------------------------------------- ## .imp: 3 ## [1] 38.998333 5.391361 Reduce(&quot;+&quot;,desc)/length(desc) ## [1] 38.973889 5.374183 4.3 Pooling Correlation coefficients When a normal distribution of the parameter estimates cannot be assumed, like for the correlation coefficients, a Fishers Z transformation has to be performed before pooling (see the appendix for the formula’s). This is automatically done in SPSS and R. 4.3.0.1 Pooling Correlation coefficients in SPSS A pooled Pearsons correlation coefficient between for example, the Tampascale and Age variables can be extracted using Analyse -&gt; Correlate -&gt; Bivariate. Than transfer the variable Tampa scale and Age to the variables window and click on OK. The pooled results are shown in (Figure 4.6), in the row called Pooled. The pooled correlation is 0.255, and the significance level is 0.002. These correlations are calculated using Fishers Z transformation before pooling and after pooling they are back-transformed. Figure 4.6: Pearson correlation between the Tampascale variable and Age. 4.3.0.2 Pooling Correlation Coefficients in R You can use the micombine.cor function in the miceadds package to obtain pooled correlation coefficients. # Read in the dataset dataset &lt;- read.spss(file=&quot;Backpain 150 missing.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 # Impute missing data using the mice function, with printFlag is F(alse), # which means that the imp and iter information is hided (called silent # computation) imp &lt;- mice(dataset, m=3, maxit=50, seed=2375, printFlag=F) # Run the micombine.cor function for the variables in column 2 # and 5, i.e. variables Tampascale and Age res.mi.cor &lt;- micombine.cor(mi.res=imp, variables = c(2,5) ) res.mi.cor ## variable1 variable2 r rse fisher_r fisher_rse ## 1 Tampascale Age 0.252762 0.07748072 0.2583611 0.08278965 ## 2 Age Tampascale 0.252762 0.07748072 0.2583611 0.08278965 ## fmi t p lower95 upper95 ## 1 0.007555655 3.120693 0.001804258 0.09580167 0.3974575 ## 2 0.007555655 3.120693 0.001804258 0.09580167 0.3974575 # Ouput of the micombine.cor function, with in the columns: # r: Pooled Pearsons correlation coefficient. # rse: Standard error of pooled correlation. # fisher_r: Transformed pooled r # fisher_rse: Standard error of transformed pooled r # fmi: Fraction of missing information. # t: T-value. # p: P-value. # lower95 and upper95: 95% lower and upper confidence intervals. 4.3.1 The Pooled Independent T-test To pool the independent t-test, Rubin´s Rules can be used. These steps are discussed in detail, including the formula’s to get the results, in Chapter 9. 4.3.1.1 Pooling Independent T-tests in SPSS To get a pooled t-test result to estimate the difference in mean Tampascale values between patients with and without Radiation in the leg you go to: Analyze -&gt; Compare Means -&gt; Independent-Samples T Test Transport the Tampa Scale variable to the Test Variable(s) window and the Radiation variable to the Grouping Variable window. Than Click on Define Groups and Define Group 1 as “1” and Group 2 as “0”. Than Click on Continue and OK. The following output table will show up, Figure 4.7. Figure 4.7: T-test for difference in mean Tampascale values between patients with and without Radiation in the leg applied in multiple imputed datasets. Figure 4.8: b.T-test for difference in mean Tampascale values between patients with and without Radiation in the leg applied in multiple imputed datasets. The result in the original dataset (including missing values) is presented in the row that is indicated by Imputation_ number 0. Results in each imputed dataset are shown in the rows starting with number 1 to 3. In the last row which is indicated as “Pooled”, the summary estimates of the mean differences, standard errors, p-values and 95% Confidence Interval are presented. 4.3.1.2 Pooling Independent T-tests in R with mice The mice package itself does not have a pooled t-test option. Instead a linear regression analysis has to be conducted. A linear regression analysis with a continuous outcome variable and an independent dichotomous variable is the same procedure as an independent t-test. Use for this the lm procedure in mice with as independent variable Radiation and dependent variable Tampascale. # Reading in the dataset library(foreign) library(mice) dataset &lt;- read.spss(file=&quot;Backpain 150 missing.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 # Impute the missing values using the mice function imp &lt;- mice(dataset, m=3, maxit=50, seed=2375, printFlag=F) # Conduct an independent t-test via lm in each imputed dataset fit.t.test &lt;- with(data=imp, exp=lm(Tampascale ~ Radiation)) t.test.estimates &lt;- pool(fit.t.test) summary(t.test.estimates) ## estimate std.error statistic df p.value ## (Intercept) 38.153846 0.5628260 67.78977 141.2188 0.00000000 ## Radiation 1.993047 0.9205375 2.16509 106.3217 0.03206084 We see in the output, under est and se the same values as in SPSS (Figure 4.6), the pooled value of 1.97 and 0.92 for the mean difference and standard error respectively. Under the column df in R you see that the dfs for the mean differences in the Tampascale variable are much smaller than those in (Figure 4.6) above. This is due to the different formulas used to calculate the df. SPSS uses an older version and mice an adjusted one (see Chapter 9 for more information about different ways to calculate the df between SPSS and R) 4.3.1.3 Pooling Independent T-tests in R with mi.t.test you can also use the mi.t.test function in the MKmisc package. Note that the mi.t.test function uses the parameter setting var.equal is True when equal variances are assumed and var.equal is False when equal variances are not assumed (the default setting is var.equal is False). # Read in the dataset dataset &lt;- read.spss(file=&quot;Backpain 150 missing.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 # Use the mice function to impute the missing data imp &lt;- mice(dataset, m=3, maxit=50, seed=2375, printFlag=F) # Extract the imputed datasets and define the Radiation variable # as a factor variable dataset1 &lt;- complete(imp,1) dataset1$Radiation &lt;- factor(dataset1$Radiation) dataset2 &lt;- complete(imp,2) dataset2$Radiation &lt;- factor(dataset2$Radiation) dataset3 &lt;- complete(imp,3) dataset3$Radiation &lt;- factor(dataset3$Radiation) # Assign the imputed datasets to the list object dataset.imp dataset.imp &lt;- list(dataset1, dataset2, dataset3) # Start the MKmisc library and run the mi.t.test function to get pooled # results of the t-test library(MKmisc) # Result of the pooled t-test mi.t.test(dataset.imp, x = &quot;Tampascale&quot;, y = &quot;Radiation&quot;, var.equal = T) ## ## Multiple Imputation Two Sample t-test ## ## data: Variable Tampascale: group 0 vs group 1 ## t = -2.1651, df = 106.32, p-value = 0.03262 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.8180378 -0.1680552 ## sample estimates: ## mean (0) SD (0) mean (1) SD (1) ## 38.153846 5.597451 40.146893 5.198414 With the mi.t.test function also a one sample and a paired t-test can be conducted. 4.4 Pooling Chi-square tests 4.4.1 Pooling Chi-square tests in SPSS The pooling of Chi-square values as a result of the Chi-square test is not available in SPSS. This lack of reporting of the Chi-Square test is shown in (Figure 4.9) where the association between the Tampa scale variable as a categorical variable (with the categories 0 = low fear of movement, 1 = middle fear of movement and 2 is a high fear of movement) and Radiation in the leg is studied. The Chi-square test is presented in the original dataset and in each imputed dataset, but a pooled Chi-square value and pooled p-value is missing. This is remarkable because when you choose for Descriptive Statistics -&gt; Crosstabs to conduct the Chi-square test the special Multiple Imputation icon is shown. This is an indication that you get pooled results, however in this case it is not. Figure 4.9: Chi-square test in 5 imputed dataset to test the relationship between the Tampascale variable and Radiation, where a pooled estimate is missing. 4.4.2 Pooling Chi-square tests in R Procedures to pool Chi-square values are available in the miceadds package. The pooling functions are based on formulas that can be found in Marshall (2009) and Enders (2012) and are referred to as the D2 statistic. To pool the Chi-square values of the SPSS example you use: library(miceadds) micombine.chisquare(c(1.829, 1.311, 2.861, 1.771, 3.690), 2, display = TRUE, version=1) ## Combination of Chi Square Statistics for Multiply Imputed Data ## Using 5 Imputed Data Sets ## F(2, 240.99)=0.869 p=0.42056 The function micombine.chisquare also has a parameter setting that is called “version”. The default version=1 refers to the correct formula as in Enders (2010), while version=0 uses an incorrect formula as printed in Allison (2001). 4.5 Analysis of Variance (ANOVA) pooling 4.5.1 Analysis of Variance (ANOVA) pooling in SPSS The pooling of Analysis of Variance (ANOVA) statistics is not available in SPSS. In Figure 4.10 the table is shown as a result of ANOVA after multiple imputation. It is clear from the Figure that the pooled results are lacking. Figure 4.10: ANOVA in SPSS without a pooled result. 4.5.2 Analysis of Variance (ANOVA) pooling in R The pooled ANOVA procedure uses the same function as was used in the previous paragraph to derive the pooled Chi-square value, because the Chi and the F-value are related. The easiest way to obtain a p-value for the ANOVA is by using the mi.anova function in the miceadds package. In this function a regression based formula can be defined to get a p-value. To compare the Function means between three Tampascale variable groups, you use: # Read in the dataset dataset &lt;- read.spss(file=&quot;Backpain 150 Missing_Tampa_Cat.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 # Generate 5 impued datasets # and set printFlag = F for a silent imputation imp.Tampa.cat &lt;- mice(dataset, m=5, maxit=50, seed=2345, printFlag = F) # Apply the mi.anova function library(miceadds) mi.anova(mi.res=imp.Tampa.cat, formula=&quot;Function ~ Tampa_Cat&quot; ) ## Univariate ANOVA for Multiply Imputed Data (Type 2) ## ## lm Formula: Function ~ Tampa_Cat ## R^2=0.1494 ## .......................................................................... ## ANOVA Table ## SSQ df1 df2 F value Pr(&gt;F) eta2 partial.eta2 ## Tampa_Cat 427.7156 1 100.1874 20.5678 2e-05 0.14943 0.14943 ## Residual 2434.6298 NA NA NA NA NA NA The pooled F and p-values are reported under the columns F value and Pr(&gt;F) respectively. 4.6 Pooling Regression models 4.6.1 Pooling Linear Regression Models in SPSS To pool the results from a linear regression analysis Rubin´s Rules are used. To study the relationship between the Tampascale (independent) and Function (dependent) variables go to: Analyze -&gt; Regression -&gt; Linear. Transport the variable Function to the Dependent window and the Tampa scale variable to the Independent(s) window. To get pooled 95% Confidence Intervals, go to Statistics and select the Confidence Intervals option. Than click on Continue and OK. Figure 4.11: Relationship between Tampascale and Function estimated with linear regression in SPSS. Information is provided in the row called Pooled about the parameter estimates, i.e. regression coefficients, standard errors, t-values, p-values and confidence interval. Further, information is provided about the Fraction of Missing Information, Relative Increase Variance and Relative Efficiency. 4.6.2 Pooling Linear regression models in R A pooled linear regression analyses can be produced by using the with and pool functions in the mice package. dataset &lt;- read.spss(file=&quot;Backpain 150 missing.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 imp &lt;- mice(dataset, m=3, maxit=50, seed=3715, printFlag=F) fit &lt;- with(data=imp,exp=lm(Function ~ Tampascale)) lin.pool &lt;- pool(fit) summary(lin.pool) ## estimate std.error statistic df p.value ## (Intercept) 26.307730 3.51479517 7.484854 6.671497 0.000176412 ## Tampascale -0.375401 0.09252282 -4.057388 5.963136 0.005338392 # Results of the pooled procedure, with: # est: Pooled regression coefficient. # se: Standard error of pooled regression coefficient. # t: T-value. # df: Degrees of freedom. # Pr(&gt;|t|): P-value. # lo 95 and hi 95: 95% lower and upper confidence intervals. # nmis: number of missing observations. # fmi: fraction of missing information. # Lambda: Proportion of the variation attributable to the missing data 4.6.3 Pooling Logistic Regression models in SPSS To study the relationship between the variables Function (independent variable) and Radiation in the Leg (dependent variable we need Logistic regression. This procedure can be done in SPSS via Analyze -&gt; Regression -&gt; Binary Logistic. Transport the variable Radiation in the Leg to the Dependent window and the Function variable to the Covariates window. To get pooled 95% Confidence Intervals, go to Options and select the CI for exp(B) option. Than click on Continue and OK. Figure 4.12: Logistic Regression in SPSS. information is provided in the row called Pooled about the parameter estimates, i.e. regression coefficients (B), standard errors (S.E.), p-values (Sig.), odds ratio´s (Exp(B) and 95% confidence intervals around the OR (95% C.I. for EXP(B). Further, information is provided about the Fraction of Missing Information, Relative Increase Variance and Relative Efficiency. For the pooled coefficient and standard error Rubin´s Rules (RR) are used. 4.6.4 Pooling Logistic Regression models in R You can use mice to get pooled results after logistic regression. In combination with the pool function you have to use the following R code. dataset &lt;- read.spss(file=&quot;Backpain 150 missing.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 imp.LR &lt;- mice(dataset, m=3, maxit=50, seed=2268, printFlag = FALSE) fit &lt;- with(data=imp.LR, exp=glm(Radiation ~ Function, family = binomial)) summary(pool(fit)) ## estimate std.error statistic df p.value ## (Intercept) 0.33331305 0.54023681 0.6169758 30.48361 0.5418318 ## Function -0.06669438 0.04498345 -1.4826425 26.76133 0.1484349 # Results of the pooled procedure, with: # est: Pooled regression coefficient. # se: Standard error of pooled regression coefficient. # t: T-value. # df: Degrees of freedom. # Pr(&gt;|t|): P-value. # lo 95 and hi 95: 95% lower and upper confidence intervals. # nmis: number of missing observations. # fmi: fraction of missing information. # Lambda: Proportion of the variation attributable to the missing data Under the Line with the R code summary(pool(fit)), the pooled estimates are provided. To extract the ORs and the corresponding 95% Confidence intervals you have to apply the following code: #summary.fit &lt;- summary(pool(fit)) #pool.OR &lt;- exp(cbind(summary.fit[,1],summary.fit[,6],summary.fit[,7])) #colnames(pool.OR) &lt;- (c(&quot;OR&quot;, &quot;95% LO&quot;, &quot;95% UP&quot;)) #pool.OR Another procedure to get the pooled estimates from a logistic regression model is by using the micombine function in the mitools package. library(mitools) dataset &lt;- read.spss(file=&quot;Backpain 150 missing.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 imp &lt;- mice(dataset, m=3, maxit=50, seed=2268, printFlag = F) dataset1 &lt;- complete(imp,1) dataset2 &lt;- complete(imp,2) dataset3 &lt;- complete(imp,3) dataset.imp &lt;- list(dataset1, dataset2, dataset3) imp.LR &lt;- lapply(dataset.imp, function(x) { glm(Radiation ~ Function, family = binomial, data = x) }) coef &lt;- MIextract(imp.LR, fun=coef) se &lt;- MIextract(imp.LR, fun=vcov) summary.fit &lt;- summary(MIcombine(coef, se) ) ## Multiple imputation results: ## MIcombine.default(coef, se) ## results se (lower upper) missInfo ## (Intercept) 0.33331305 0.54023681 -0.7572339 1.42385997 25 % ## Function -0.06669438 0.04498345 -0.1579936 0.02460484 28 % pool.OR &lt;- exp(summary.fit[, -c(2, 5)]) colnames(pool.OR) &lt;- (c(&quot;OR&quot;, &quot;95% LO&quot;, &quot;95% UP&quot;)) pool.OR ## OR 95% LO 95% UP ## (Intercept) 1.3955841 0.4689618 4.15312 ## Function 0.9354811 0.8538552 1.02491 However, the pooled p-value is still missing. You can get the pooled p-values from the mi.inference function in the NORM package. library(norm) se &lt;- lapply(se, function(x) sqrt(diag(x)) ) res &lt;- mi.inference(coef,se) res.pool &lt;- matrix(unlist(res), 2, 8, byrow=F) colnames(res.pool) &lt;- names(res) rownames(res.pool) &lt;- names(res$est) res.pool ## est std.err df signif lower ## (Intercept) 0.33331305 0.54023681 41.60735 0.5406124 -0.7572339 ## Function -0.06669438 0.04498345 35.23806 0.1470584 -0.1579936 ## upper r fminf ## (Intercept) 1.42385997 0.2808117 0.2542508 ## Function 0.02460484 0.3127440 0.2780801 pool.OR &lt;- exp(res.pool[, -c(2:4, 7, 8)]) colnames(pool.OR) &lt;- (c(&quot;OR&quot;, &quot;95% LO&quot;, &quot;95% UP&quot;)) pool.OR ## OR 95% LO 95% UP ## (Intercept) 1.3955841 0.4689618 4.15312 ## Function 0.9354811 0.8538552 1.02491 The p-value in the NORM package is equal to the p-value in SPSS. This means that the NORM package also uses the older method to calculate the degrees of freedom. 4.7 Pooling logistic regression models including categorical independent variables For categorical variables in logistic regression models, different methods are available to test the variable as a whole for significance. These so called Multiparameter tests are not available in SPSS, but they are available in R. These tests will be discussed in the next Chapter. We will start with an example of univariate pooling in SPSS. 4.7.1 Pooling Cox regression models One of the most used statistical models for survival data is the Cox regression model. With survival data you have two outcome measures, the status variable and the time to event variable. As a guideline, all variables of the main analysis, including the outcome variable have to be part of the imputation model. The best way to include the outcome variable in a Cox regression model is not by using the Time variable itself, but by using the cumulative hazard to the survival time. This value has to be included in the imputation model together with the status variable and the auxiliary variables. 4.7.2 Pooling Cox regression models in SPSS The cumulative hazard value can easily be calculated in SPSS by using the Survival menu and then choose for Analyze -&gt; Cox Regression Figure 4.13: The survival options in SPSS. Than choose for Save and the following window will open. Figure 4.14: The Save menu under Cox regression. Here you can choose for Hazard function. Then click on Continue and OK. A new variable will we added to the dataset, which is called HZA_1. This cumulative hazard variable can be included in the imputation model to impute missing data in the Pain variable. The pooling of the Cox regression model will be done in the datasets that are imputed in R. Than we can compare the output from the pooled model in SPSS and in R. To get pooled results of Cox regression models you use: Analyze -&gt; Survival -&gt; Cox Regression Transport the survival time variable to Time window, the event variable to the Status window and the independent variable Pain to the Covariates window. To get pooled 95% Confidence Intervals, go to Options and select the CI for exp(B) option. Than click on Continue and OK. Figure 4.15: The pooled Cox regression model estimated in SPSS. This procedure provides a pooled value for the regression coefficient, standard error, p-value (of 0.000589), hazard ratio and related 95% confidence intervals and provides information about the fraction of missing information, the relative increase in variance and the relative efficiency. 4.7.2.1 Pooling Cox regression models in R For this procedure we can make use of the pool function that is available in the mice package. We start by using the mice function to impute missing data in the Pain variable by first calculating the cumulative hazard values. After that we customize the predictorMatrix so that the Time variable is not used to predict the missing values (we use the cumulative hazard function instead) in the Pain variable and subsequently the imputed datasets will be pooled to get a summary estimate. Note that you also have to activate the package survival before you can run the coxph function in R. # Read in the dataset library(survival) dataset &lt;- read.spss(file=&quot;Backpain 150 Survival Missing.sav&quot;, to.data.frame=T) ## re-encoding from UTF-8 # Compute the cumulative hazard, attach it to the dataset # and omit the ID variable (first column) Hazard &lt;- nelsonaalen(dataset, Time, Status) dataset &lt;- data.frame(dataset, Hazard) # Adapt the PredictorMatrix so that the # Time variable is not included in the imputation model Cox.imp &lt;- mice(dataset, m=1, maxit=0, seed=2795, printFlag=F) Pred &lt;- Cox.imp$predictorMatrix Pred[2, &quot;Time&quot;] &lt;- 0 Pred ## Time Status Pain Tampascale Function Radiation Hazard ## Time 0 1 1 1 1 1 1 ## Status 0 0 1 1 1 1 1 ## Pain 1 1 0 1 1 1 1 ## Tampascale 1 1 1 0 1 1 1 ## Function 1 1 1 1 0 1 1 ## Radiation 1 1 1 1 1 0 1 ## Hazard 1 1 1 1 1 1 0 # Start imputations using mice Cox.imp &lt;- mice(dataset, m=3, maxit=50, predictorMatrix=Pred, seed=2795, printFlag=F) fit.Cox &lt;- with(data=Cox.imp, exp=coxph(Surv(Time, Status) ~ Pain)) Cox.pool &lt;- pool(fit.Cox) ## Warning: Unknown or uninitialised column: &#39;df.residual&#39;. ## Warning in pool.fitlist(getfit(object), dfcom = dfcom): Large sample ## assumed. summary(Cox.pool) ## estimate std.error statistic df p.value ## Pain -0.2021694 0.04789423 -4.221165 36823.71 2.436252e-05 # Results of the pooled procedure, with: # est: Pooled regression coefficient. # se: Standard error of pooled regression coefficient. # t: T-value. # df: Degrees of freedom. # Pr(&gt;|t|): P-value. # lo 95 and hi 95: 95% lower and upper confidence intervals. # nmis: number of missing observations. # fmi: fraction of missing information. # Lambda: Proportion of the variation attributable to the missing data The value of 0.3319019 in the column named fmi for the Pain variable is calculated according to the Formula 5.17 for FMI. The value 0.2811114 under the column named lambda is calculated according to Formula 5.13. "],
["advanced-multiple-imputation-models-for-multilevel-data.html", "Chapter 5 Advanced Multiple Imputation models for Multilevel data 5.1 Characteristics of Multilevel data 5.2 Example Multilevel dataset 5.3 Multilevel data - from wide to long", " Chapter 5 Advanced Multiple Imputation models for Multilevel data In this Chapter, we apply more advanced imputation models. With “advanced”, we mean multiple imputation models for multilevel data, which are also called mixed models. We start this Chapter with a brief introduction about multilevel data. Subsequently, we will shortly discuss the basic principles of mixed models. After that, we will discuss the levels of missing data that you can encounter when you have a multilevel dataset and we will show some examples of how to apply multilevel imputation models. 5.1 Characteristics of Multilevel data Multilevel data is also known as clustered data, where collected data is clustered into groups. Examples are observations that are repeatedly assessed within persons over time, assessments of patients within the same hospitals or observations of students within the same schools. We say that these data are clustered because assessments of patients or students within the same hospital or school are more equal to each other than assessments of patients or students between different hospitals or schools (Twisk 2006). It is called multilevel data because data is assessed at different levels. Data can be assessed at the level of the school when we would be interested in the school type, i.e. private or public school. The data is than assessed at two levels, i.e. the school level (highest level or level 2) and the students level (lowest level or level 1). Another example is when blood parameters or variables as bodyweight are repeatedly assessed within the same individuals (clusters) over time. Here the clusters are the individuals. This type of data is also called longitudinal data. Assessments within the same individual may be more alike than assessments between individuals. This kind of data is also assessed at two levels, now the individuals are the highest level (level 2) and the time measurements are the lowest level (level 1). Multilevel data may also consist of data assessed at more than 2 levels, i.e. data that is assessed in different schools, classes and students or different regions, hospitals and patients. 5.2 Example Multilevel dataset In this Chapter we use for the examples about longitudinal missing data The Amsterdam Growth and Health Longitudinal Study (AGGO). In this study persons were repeatedly assessed over time and growth, health and life-style factors were measured. Assessments are available of Gender, Fitness, Smoking, Hypercholestrolemia, Cholesterol level and Sum of Skinfolds. The dataset contains information of 147 patients which are assessed six times, once at baseline and at 5 repeated measurement occasions. 5.3 Multilevel data - from wide to long Usually, a dataset contains one row per subject and the separate variables are placed in the columns. When subjects are repeatedly assessed, additional variables are added for new assessments. This is also called a wide data format (Figure 5.1). In this dataset, information is repeatedly assessed over time for the Cholesterol and Sum of Skinfold variables and this information is stored in the column variables. The repeated assessments are distinguished by the numbers at the end of the variable names. Number 1 indicates the first measurement, number 2, the second, etc. The Gender and Smoking variable is not repeatedly assessed over time. Figure 5.1: Example of the AGGO dataset in wide format. In order to apply multilevel imputation analyses, we need a long version of the data. We have to convert the dataset into a long data format. "],
["restructuring-dataset-from-wide-to-long-in-spss.html", "Chapter 6 Restructuring dataset from wide to long in SPSS 6.1 Multilevel data - Clusters and Levels 6.2 The Multilevel model 6.3 Missing data at different levels 6.4 Multilevel imputation models 6.5 One stage Multilevel Imputation 6.6 Missing data at different levels 6.7 Multilevel imputation models 6.8 Two stage Multilevel Imputation 6.9 The micemd Package 6.10 Pooling Multilevel models 6.11 Pooling GEE models", " Chapter 6 Restructuring dataset from wide to long in SPSS Click on Data Restructure and follow the next steps: Step1 A new window opens with three options (Figure 6.1). We use the default setting to Restructure selected variables into cases. Figure 6.1: Step 1 of the Restructure Data Wizard. Step 2 Click Next and a new window opens (Figure 6.2). In this window, you choose how many (level 1) variables you wish to restructure. We have two such variables: Cholesterol and Sum of Skinfolds. Therefore, we click the option More than one and type 2. Figure 6.2: Step 2 of the Restructure Data Wizard. Step 3 We click Next and a new window opens (Figure 6.2). In this window, you define which variable is the case group identifier. SPSS by default makes a new variable for this, named Id. We use the ID variable in the dataset (you usually have such a variable): by clicking the arrow next to the Use case number option, you select Use selected variable and after that drag the ID variable in the dataset to this pane. Subsequently, you define the variables in the broad dataset are restructured to one new long variable under Variables to be transposed. This refers to two new variables. Rename trans1 into Cholesterol and select the 6 Cholesterol variables by holding down the Ctrl or Shift key. Next, move these variables to the pane on the right and continue with the second variable (Figure 6.3). Now change trans2 into SumSkinfolds and repeat the procedures for the Sum of Skinfolds variables. Figure 6.3: Step 3 of the Restructure Data Wizard. Step 4 Click Next and a new window opens (Figure 6.4). In this window, you can create an Index variables. The index variable refers to the time points. You need one index variable, which is the default setting in the window, so you can click Next again. Figure 6.4: Step 4 of the Restructure Data Wizard. Step 5 A new window opens again (Figure 6.5). This window allows us to create the index/time variable. The default is to use sequential numbers, which you choose. In case of unequal time points you can redefine these numbers later in the long data file with the Compute command in SPSS. Change the name index1, by double clicking on it. Rename it in “Time”. In addition we can define a label for this variable in the same way. Figure 6.5: Step 5 of the Restructure Data Wizard. Step 6 Click Next and a new window opens again (Figure 6.6). Here the only important thing is that we should choose what to do with the other variables in our dataset. We can either Drop them, meaning that we will not be able to use them in the subsequent analyses, or Keep them and treat them as fixed (time independent). In this case choose for this latter option. Figure 6.6: Step 6 of the Restructure Data Wizard. Click on Next and the last window will open (Figure 6.7). Figure 6.7: Last step of the Restructure Data Wizard. This is the final step. Click on Finish (if we wish to paste the syntax we can choose for that here). Be aware that when you click on finish the converted dataset replaces the original dataset To keep both datasets, use Save as in the menu file and choose another file name for the converted file (Figure 6.8). Figure 6.8: Example of the AGGO dataset in long format. The variable that separates the clusters is the ID variable and the variable that distinguishes the measurements at different time points is the Time variable. This means that repeated assessment within a subject are stacked under each other. Each subject has multiple rows, one row for each repeated measurement. 6.0.1 Restructuring a dataset from wide to long in R To convert a dataset in R from wide to long, you can use the reshape function. Before you convert the dataset, it is a good idea to redesign the dataset a little bit and to place all variables in the order of their names. It is than easier to apply the reshape function. You see an example in the R code below, where all Cholesterol variables are nicely ordered. library(foreign) dataset &lt;- read.spss(file=&quot;AGGO_wide.sav&quot;, to.data.frame = T) ## re-encoding from UTF-8 head(dataset, 10) ## ID Gender Fitness Smoking Cholesterol1 Cholesterol2 Cholesterol3 ## 1 1 1 2.151339 0 4.2 3.9 3.9 ## 2 2 1 2.119814 0 4.4 4.2 4.6 ## 3 3 1 2.472159 0 3.7 4.0 3.3 ## 4 4 1 2.205836 0 4.3 4.1 3.8 ## 5 5 1 2.393320 0 4.2 4.1 4.1 ## 6 6 1 2.523713 0 4.1 3.8 3.6 ## 7 7 1 2.260524 0 4.2 3.9 3.7 ## 8 8 2 1.915354 0 3.2 3.7 3.6 ## 9 9 2 2.278520 0 5.1 4.6 3.9 ## 10 10 2 2.002636 1 5.7 5.4 5.3 ## Cholesterol4 Cholesterol5 Cholesterol6 SumSkinfolds1 SumSkinfolds2 ## 1 3.6 3.92 4.13 2.51 2.10 ## 2 4.1 5.18 5.50 2.48 2.34 ## 3 3.6 3.48 3.85 2.17 2.39 ## 4 3.4 4.35 4.06 2.58 2.68 ## 5 4.5 3.81 4.52 2.33 2.02 ## 6 3.7 3.57 4.24 2.05 2.09 ## 7 3.6 4.32 5.08 2.42 2.93 ## 8 3.2 3.82 4.42 3.44 3.78 ## 9 4.5 4.22 5.27 2.43 2.72 ## 10 4.9 5.24 5.34 4.12 4.83 ## SumSkinfolds3 SumSkinfolds4 SumSkinfolds5 SumSkinfolds6 ## 1 2.16 2.26 2.33 2.56 ## 2 2.45 2.57 3.97 5.08 ## 3 2.44 2.42 2.80 4.48 ## 4 2.84 3.00 2.85 2.77 ## 5 2.00 2.27 1.94 2.16 ## 6 2.25 2.24 2.66 2.73 ## 7 3.14 3.35 3.90 4.01 ## 8 4.78 4.86 5.49 6.25 ## 9 3.06 3.72 5.28 5.11 ## 10 6.21 6.35 5.99 3.70 ## Hypercholestrolemia ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 ## 7 0 ## 8 0 ## 9 1 ## 10 1 Now it is easy to convert the dataset by using the following code. The object dataset_long shows the results (first 10 patients shown): # Reshape wide to long dataset_long &lt;- reshape(dataset, idvar = &quot;ID&quot;, varying = list(5:10, 11:16), timevar=&quot;Time&quot;, v.names = c(&quot;Cholesterol&quot;, &quot;SumSkinfolds&quot;), direction = &quot;long&quot;) #dataset_long The long dataset is not ordered yet by ID and Time. This can be done by using the order function. dataset_long &lt;- dataset_long[order(dataset_long$ID, dataset_long$Time), ] #dataset_long Now that we have restructured the dataset we are going to discover how missing data in multilevel data can be imputed. 6.1 Multilevel data - Clusters and Levels In the previous paragraph we have seen an example of a long dataset that is needed for multilevel analyses. We can organize this kind of multilevel information by level of assessment in the following way (Figure 6.9): Figure 6.9: 7.4. AGGO dataset with level 1 and 2 variables. Level 1 outcome variable: This is for example the Cholesterol variable that is repeatedly assessed over time within persons. Other examples may be math test scores of individual students in a class or their Intelligent Quotient (IQ) scores. In other words, level one outcome information varies within a cluster or the value changes over time (i.e. does not have a fixed value). Level 1 independent variable: These are variables that vary within a cluster, but now are used as independent variables. Examples are the Time or Sum of Skinfold measurements that are repeatedly assessed within persons over time or hours that students in a class spent on their homework each week or the level of education of their parents. Level 2 independent variables. These variables do not vary within a cluster but vary only between clusters. An example is the Gender or Fitness variable which is only assessed at the start of the study, or in case of schools, if the school is a private or public school. Level 4 is the cluster variable itself. This is a special variable which distinguishes the clusters. This could be the school identification number which form the blocks of measurement or the identification number that distinguishes individuals with repeated information over time. In the next paragraph the type of statistical model is defined that is used to analyze multilevel data. 6.2 The Multilevel model In the analysis model of multilevel data, the clustered structure of the data has to be accounted for. In other words, the model corrects for the correlations within clusters. This is done by adding a random effect to the model, additional to the fixed effects. Fixed effects are the regression coefficients that are estimated as in normal regression models as fixed intercept or slope effects. They estimate the average effect of a whole group. The random effects are the cluster specific effects and can be embedded in the model as random intercept and random slope effects. Accordingly, multilevel models contain a “mix” of both random and fixed effects and are therefore often referred to as mixed effects models or random effects models. The multilevel model can be defined as depicted in (Figure 6.10): Figure 6.10: Definition of Multilevel model. The i indicates the different clusters and the j the repeated measurements within each cluster. Yij: is the level 1 outcome variable. Xij: is the level 1 or 2 independent variable. B: is the fixed effect regression coefficient (for the intercept and the slope). bi: is the random effect regression coefficient (for the intercept and the slope). eij: is the error variance. For example, if it would be of interest to examine the relationship between the Sum of Skinfolds and Cholesterol over time using the information in (Figure 6.3), the analysis model would be: \\[Cholesterol_{ij} = \\beta_0 1 + Time_{ij}\\beta_1 + SumSkinfolds_{ij}\\beta_2 + b_i + \\varepsilon_i\\] With the outcome and independent variables as level 1 variables in the model. In the next paragraph we explain how to restructure a dataset from wide to long in SPSS and R, needed to analyze multilevel data. We use the longitudinal AGGO data as an example. 6.3 Missing data at different levels Missing data in multilevel studies can occur at the same levels as measurements as was discussed in paragraph 7.3. In other words, missing data can occur at the level of: The Level 1 outcome variable: Missing data is present in the Cholesterol variable when this variable is repeatedly assessed over time or in math scores of pupils within a class. Note that when Mixed models are used and there is only missing data in the outcome variable, imputation of missing values is not necessary. Full information maximum likelihood procedures, that are used to estimate the parameters of a mixed model, can be used to get estimates of regression coeficients and standard errors. The Level 1 independent variable: Missing data occur at the level of the independent variables that vary within a cluster. Examples are missing data in the Sum of Skinfold variable or the age or IQ scores of students within a class. The Level 2 independent variables: Missing values are located in the variables that have a constant value within a cluster. For example, in the Fitness variable assessed at baseline or if data is missing for the variable if a school is a private or public school. Other examples are when data is missing for variables as gender or educational level of persons that were repeatedly assessed over time. The Cluster level variable: Missing data may be present in the cluster variable itself, for example if students did not fill in the name of the school or patients did not fill in the name of the hospital they were treated. 6.4 Multilevel imputation models In order to deal with missing values in multilevel data, the structure of the data needs to be accounted for. In Chapter 4 we have described that the imputation model for multiple imputation, needs to be compatible to the analysis model. Since the analysis model for multilevel data, uses random effects, the imputation model should ideally contain these random effects as well. Accordingly, multilevel imputation model need to be used, which are more complicated than non-multilevel imputation model. Several procedures are available for multilevel imputation that are compatible with the mice package. In this Chapter, we will use two procedures. One procedure is the application of a one stage imputation method using fully conditional specification (FCS), also called the FCS-GLM approach. The other is a two-stage multilevel imputation procedure using fully conditional specification, which is called the FCS-2stage procedure. Both procedures are available in the micemd package (Audigier, 2017) and follow the MICE procedure to impute missing values. Accordingly, a chain of multilevel imputation models is used and a lot of parameter settings that are used in the mice function can also be used to impute multilevel data. The FCS-2stage procedure slightly differs from the FCS-GLM procedure, because imputations are generated in two steps. The procedures will be discussed in more detail with examples of how to apply them. As was discussed in Chapter 4, for missing data, Bayesian estimates are used in the imputation model to estimate the missing values. The imputed values are drawn from the posterior predictive distribution, conditional on the values of other variables. These procedures are also incorporated in the multilevel imputation procedures in the micemd package. The steps that are used to impute missing multilevel data are mathematically complex and we will therefore only explain these steps conceptually. Readers who are more interested in the mathematical details are referred to the papers of Jolani et al (2015), Resche-Rigon &amp; White (2016) and Audigier et al (to appear). 6.4.1 Sporadically and Systematically missing data In paragraph 7.5 it was discussed that missing data in multilevel datasets can occur at different levels. Another distinction can be made between sporadically and systematically missing values. Sporadically missing values are values that are sometimes missing within a cluster and systematic missing data is when all values within a cluster are missing. In (Figure 6.11). an example of sporadically and systematically missing data in the Sum of Skinfolds variable is presented. For patient number 1 and 2 the values are sporadically missing, i.e. some values are missing within the cluster of measurements of patients 1 and 2 and for patient number 3 the data is systematically missing, i.e. all Sum of Skinfold values are missing. In the next paragraph, solutions are presented of both types of missing data. Figure 6.11: Example of Sporadically and systematic missing data in the Sum of Skinfolds variable. For ID number 1 and 2 some values are (sporadically) missing and for ID number 3 all Sum of Skinfolds data is (systematically) missing. 6.5 One stage Multilevel Imputation One stage Multilevel imputation can be performed in R by using the micemd package. This package is compatible with the MICE package and in this package several procedures are available for multilevel imputation. The one stage Fully Conditional Specification Generalized Linear Mixed (FCS-GLM) imputation procedure is one of these methods. The FCS-GLM procedure uses the function mice.impute.2l.glm.norm and works best when there are few clusters and not so many individuals per cluster (see Box 2 below). This procedure follows roughly the same route as for Bayesian stochastic regression imputation only now linear mixed models are used to generate the imputations. For more information about these procedures see the papers of Jolani et al (2015), Resche-Rigon &amp; White (2016) and Audigier (to appear). With the one stage imputation procedure missing values are imputed using basically the following steps: 1. A random effects multilevel model is estimated in the available data to determine the regression coefficients, between cluster variance-covariance matrix and the residual (within cluster) error variance. 2. (Bayesian) estimates of the parameters estimated at step 1 are obtained, drawn from their posterior distribution. This means that error variation is added to the regression coefficients, variance-covariance matrix and residual error variance. 3. Cluster specific regression coefficients are derived using the Bayesian estimates of step 2. 4. These cluster specific effects are used to generate the imputed values using cluster specific regression models. The FCS-GLM procedure assumes that the within cluster error variance is constant (i.e. has the same value) over all clusters. Systematic and sporadically missing values can be imputed by the one stage FCS-GLM procedure. 6.5.1 One stage Multilevel Imputation in R We will now show how to impute missing data by using the FCS-GLM procedure in the longitudinally measured AGGO dataset. In the dataset, missing values exist in the independent Sum of Skinfolds variable and the outcome variable is Cholesterol. Both these variables are repeatedly assessed. There are also other variables in the dataset, as Gender, and Fitness score which can be used as auxiliary variables to improve the imputations. ID is the cluster variable. For this example, there is sporadically missing data in de dataset within the clusters 1, 2, 4 and 5 and systematically missing data in cluster 3, which means that within clusters 1, 2, 4 and 5, for 1 or 2 some repeated measured values are missing and that within cluster 3 all values are missing. The longitudinal dataset is (the first 5 persons are shown): dataset &lt;- read.spss(file=&quot;Aggo_long_missing.sav&quot;,to.data.frame = T) ## re-encoding from UTF-8 #dataset The long dataset is not ordered yet by ID and Time. This can be done by using the order function: dataset_long &lt;- dataset_long[order(dataset_long$ID, dataset_long$Time), ] #dataset_long Now that we have restructured the dataset we are going to discover how missing data in multilevel data can be imputed. 6.6 Missing data at different levels Missing data in multilevel studies can occur at the same levels as measurements as was discussed in paragraph 7.3. In other words, missing data can occur at the level of: The Level 1 outcome variable: Missing data is present in the Cholesterol variable when this variable is repeatedly assessed over time or in math scores of pupils within a class. Note that when Mixed models are used and there is only missing data in the outcome variable, imputation of missing values is not necessary. Full information maximum likelihood procedures, that are used to estimate the parameters of a mixed model, can be used to get estimates of regression coeficients and standard errors. The Level 1 independent variable: Missing data occur at the level of the independent variables that vary within a cluster. Examples are missing data in the Sum of Skinfold variable or the age or IQ scores of students within a class. The Level 2 independent variables: Missing values are located in the variables that have a constant value within a cluster. For example, in the Fitness variable assessed at baseline or if data is missing for the variable if a school is a private or public school. Other examples are when data is missing for variables as gender or educational level of persons that were repeatedly assessed over time. The Cluster level variable: Missing data may be present in the cluster variable itself, for example if students did not fill in the name of the school or patients did not fill in the name of the hospital they were treated. 6.7 Multilevel imputation models In order to deal with missing values in multilevel data, the structure of the data needs to be accounted for. In Chapter 4 we have described that the imputation model for multiple imputation, needs to be compatible to the analysis model. Since the analysis model for multilevel data, uses random effects, the imputation model should ideally contain these random effects as well. Accordingly, multilevel imputation model need to be used, which are more complicated than non-multilevel imputation model. Several procedures are available for multilevel imputation that are compatible with the mice package. In this Chapter, we will use two procedures. One procedure is the application of a one stage imputation method using fully conditional specification (FCS), also called the FCS-GLM approach. The other is a two-stage multilevel imputation procedure using fully conditional specification, which is called the FCS-2stage procedure. Both procedures are available in the micemd package (Audigier, 2017) and follow the MICE procedure to impute missing values. Accordingly, a chain of multilevel imputation models is used and a lot of parameter settings that are used in the mice function can also be used to impute multilevel data. The FCS-2stage procedure slightly differs from the FCS-GLM procedure, because imputations are generated in two steps. The procedures will be discussed in more detail with examples of how to apply them. As was discussed in Chapter 4, for missing data, Bayesian estimates are used in the imputation model to estimate the missing values. The imputed values are drawn from the posterior predictive distribution, conditional on the values of other variables. These procedures are also incorporated in the multilevel imputation procedures in the micemd package. The steps that are used to impute missing multilevel data are mathematically complex and we will therefore only explain these steps conceptually. Readers who are more interested in the mathematical details are referred to the papers of Jolani et al (2015), Resche-Rigon &amp; White (2016) and Audigier et al (to appear). 6.7.1 Sporadically and Systematically missing data In paragraph 7.5 it was discussed that missing data in multilevel datasets can occur at different levels. Another distinction can be made between sporadically and systematically missing values. Sporadically missing values are values that are sometimes missing within a cluster and systematic missing data is when all values within a cluster are missing. In Figure 7.15. an example of sporadically and systematically missing data in the Sum of Skinfolds variable is presented. For patient number 1 and 2 the values are sporadically missing, i.e. some values are missing within the cluster of measurements of patients 1 and 2 and for patient number 3 the data is systematically missing, i.e. all Sum of Skinfold values are missing. In the next paragraph, solutions are presented of both types of missing data. Figure 6.12: Example of Sporadically and systematic missing data in the Sum of Skinfolds variable. For ID number 1 and 2 some values are (sporadically) missing and for ID number 3 all Sum of Skinfolds data is (systematically) missing. 6.7.2 One stage Multilevel Imputation One stage Multilevel imputation can be performed in R by using the micemd package. This package is compatible with the MICE package and in this package several procedures are available for multilevel imputation. The one stage Fully Conditional Specification Generalized Linear Mixed (FCS-GLM) imputation procedure is one of these methods. The FCS-GLM procedure uses the function mice.impute.2l.glm.norm and works best when there are few clusters and not so many individuals per cluster (see Box 2 below). This procedure follows roughly the same route as for Bayesian stochastic regression imputation only now linear mixed models are used to generate the imputations. For more information about these procedures see the papers of Jolani et al (2015), Resche-Rigon &amp; White (2016) and Audigier (to appear). With the one stage imputation procedure missing values are imputed using basically the following steps: 1. A random effects multilevel model is estimated in the available data to determine the regression coefficients, between cluster variance-covariance matrix and the residual (within cluster) error variance. 2. (Bayesian) estimates of the parameters estimated at step 1 are obtained, drawn from their posterior distribution. This means that error variation is added to the regression coefficients, variance-covariance matrix and residual error variance. 3. Cluster specific regression coefficients are derived using the Bayesian estimates of step 2. 4. These cluster specific effects are used to generate the imputed values using cluster specific regression models. The FCS-GLM procedure assumes that the within cluster error variance is constant (i.e. has the same value) over all clusters. Systematic and sporadically missing values can be imputed by the one stage FCS-GLM procedure. 6.7.3 One stage Multilevel Imputation in R We will now show how to impute missing data by using the FCS-GLM procedure in the longitudinally measured AGGO dataset. In the dataset, missing values exist in the independent Sum of Skinfolds variable and the outcome variable is Cholesterol. Both these variables are repeatedly assessed. There are also other variables in the dataset, as Gender, and Fitness score which can be used as auxiliary variables to improve the imputations. ID is the cluster variable. For this example, there is sporadically missing data in de dataset within the clusters 1, 2, 4 and 5 and systemat ically missing data in cluster 3, which means that within clusters 1, 2, 4 and 5, for 1 or 2 some repeated measured values are missing and that within cluster 3 all values are missing. dataset &lt;- read.spss(file=&quot;Aggo_long_missing.sav&quot;,to.data.frame = T) ## re-encoding from UTF-8 #dataset The analysis model is: \\[Cholesterol_{ij} = \\beta_0 1 + Time_{ij}\\beta_1 + SumSkinfolds_{ij}\\beta_2\\] For the imputation of the Sum of Skinfolds variable we will use all variables in the analysis model and two extra variables, Gender and Fitness. The Imputation model will be: \\[SumSkinfolds_{ij} = \\beta_0 1 + Time_{ij}\\beta_1 + Cholesterol_{ij}\\beta_2 + Gender_{ij}\\beta_3 + Fitness_{ij}\\beta_4 + b_i + \\varepsilon_i\\] To impute the missing data in the Sum of Skinfolds variable we first have to adjust the Predictormatrix that is used by mice to define which variable is the cluster variable. # The value 1 is used as column location for the cluster variable ind.clust&lt;- 1 # The following code is to adjust the predictorMatrix # so that all variables are used in the imputation model # to impute the Sum of Skinfolds variable predmatrix.adj &lt;-mice(dataset,m=1,maxit=0) predmatrix.adj$pred[ind.clust,ind.clust]&lt;-0 predmatrix.adj$pred[-ind.clust,ind.clust]&lt;- -2 predmatrix.adj$pred[predmatrix.adj$pred==1]&lt;-2 predictor.matrix&lt;-predmatrix.adj$pred predictor.matrix ## ID Gender Fitness Time Cholesterol SumSkinfolds ## ID 0 2 2 2 2 2 ## Gender -2 0 2 2 2 2 ## Fitness -2 2 0 2 2 2 ## Time -2 2 2 0 2 2 ## Cholesterol -2 2 2 2 0 2 ## SumSkinfolds -2 2 2 2 2 0 The predictormatrix contains the values 0, -2 and 2. As explained in paragraph 4.5 in the column are the names of the variables that are used in the imputation model to estimate the missing data in the row variable. In our example, all variables in the columns are used to impute the Sum of Skinfolds variable. The diagonal of the matrix is always zero. All complete variables do not have to be imputed so the rows of all complete variables are always zero. A -2 is an indication of the location of the cluster variable. The ID variable in the dataset is here defined as “ID”. The value 2 means the location of the random effects variables. In the imputation model, all variables will be defined as having a fixed and a random effect. To extract a completed dataset, for example completed dataset 1, we use the complete function that is available in the MICE package. 6.8 Two stage Multilevel Imputation Now, we will use as an example the two-stage multilevel imputation procedure that is available in the MICEMD package. This package is also compatible with the MICE package. For two stage multilevel imputation the mice.impute.2l.2stage.norm function is used. With this function multilevel imputation models are applied in two stages. At the first stage regression models are estimated within each cluster and at the second stage a meta-analysis is conducted to get a pooled estimate. This pooled estimate is used to generate cluster specific coefficients and variance estimates, but than for the (Bayesian) posterior distribution. For more information about these procedures see the papers of Jolani et al (2015), Resche-Rigon &amp; White (2016) and Audigier (to appear). Roughly, the following steps are taken for a continuous variable with missing data: 1. In each cluster a linear regression model is fitted in the complete cases (complete case analysis). This produces (cluster-specific) regression coefficients and (cluster-specific) variance estimates. 2. A meta-analysis is performed with as input the cluster specific regression coefficients and variance estimates from step 1. 3. From step 2 the pooled regression coefficient and the random-effects variance-covariance matrix are obtained. 4. MICE is a Bayesian imputation procedure. A pooled regression coefficient from the posterior distribution is drawn and the same is done for the random-effects variance-covariance matrix. 5. Cluster specific regression coefficients are drawn from the posterior distribution using the (Bayesian) estimates from step 4 (the regression coefficient and random-effects variance-covariance matrix). 6. The same 2 stage procedures are repeated for the residual variance estimates to take account of different error variance estimates in each cluster, i.e. heteroscedastic error variance. This is not possible with the FCS-GLM method. 7. Cluster specific effects are used to generate the imputed values using cluster specific regression models. The difference with the FCS-GLM method described above and the FCS-2stage procedure is that the FCS-2stage procedure allows for a different residual variance within each cluster of the cluster specific imputation model (heteroskedastic variance instead of homoscedastic variance). The advantage of this method is that imputations are more correct. 6.8.1 Two stage Multilevel Imputation in R To show how to impute the missing according to the two-stage procedure we use a dataset were persons are assessed in different training centers. The dataset contains 880 measurements, of 110 persons obtained in 8 different centers. Lifestyle factors were assessed in individuals (Level 1) at different centers (Level 2). The centers are the clusters of information. Missing values are present in the independent Sum of Skinfolds variable and the outcome variable is Cholesterol. The two-stage multilevel imputation method works best in datasets with a lot of values within each cluster (see Box 2). Again, we take as an example sporadically and systematically missing data, with sporadically missing data in cluster 1 and systematically missing data in cluster 2. This means that in cluster 2 for all 110 persons the Sum of Skinfold data is missing! Before we are going to impute the missing data we first have to adjust the Predictormatrix that is used by MICE to define which variable is the cluster variable. For this we have to run the same code as in the previous paragraph. ind.clust&lt;-1 #index for the cluster variable # to obtain the predictorMatrix predmatrix.adj &lt;-mice(dataset,m=1,maxit=0) predmatrix.adj$pred[ind.clust,ind.clust]&lt;-0 predmatrix.adj$pred[-ind.clust,ind.clust]&lt;- -2 predmatrix.adj$pred[predmatrix.adj$pred==1]&lt;-2 predictor.matrix&lt;-predmatrix.adj$pred predictor.matrix ## ID Gender Fitness Time Cholesterol SumSkinfolds ## ID 0 2 2 2 2 2 ## Gender -2 0 2 2 2 2 ## Fitness -2 2 0 2 2 2 ## Time -2 2 2 0 2 2 ## Cholesterol -2 2 2 2 0 2 ## SumSkinfolds -2 2 2 2 2 0 Now we can start with imputing the missing data. #res.mice.md &lt;- mice(dataset,m=5,predictorMatrix = predictor.matrix, # method=c(&quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;2l.2stage.norm&quot;), # maxit=10, seed=23087) The completed dataset can be extracted using the complete function. #complete(res.mice.md, 1) Now the dataset can be used for further analyses. 6.9 The micemd Package In this paragraph a short description is given of some other imputation procedures that are available in the micemd package. For a full description of all procedures we refer to the reference manual of the micemd package, written by Vincent Audigier and Matthieu Resche-Rigon (2017). One stage imputation methods: mice.impute.2l.glm.bin Imputes univariate missing data using a Bayesian logistic mixed model based on non-informative prior distributions. The method is dedicated to a binary outcome stratified in severals clusters. Should be used with few clusters and few individuals per cluster. Can be very slow to perform otherwise. mice.impute.2l.glm.norm Imputes univariate missing data using a Bayesian linear mixed model based on non-informative prior distributions. The method is dedicated to a continuous outcome stratified in severals clusters. Should be used with few clusters and few individuals per cluster. Can be very slow to perform otherwise. mice.impute.2l.glm.pois Imputes univariate missing data using a Bayesian mixed model (Poisson regression) based on noninformative prior distributions. The method is dedicated to a count outcome stratified in severals clusters. Should be used with few clusters and few individuals per cluster. Can be very slow to perform otherwise. mice.impute.2l.jomo Univariate imputation by a Bayesian multivariate generalized model based on conjugate priors. Can be used for a continuous or binary incomplete variable. For continuous variables, the modelling assumes heteroscedasticity for errors. For a binary variable, a probit link and a latent variables framework are used. Two stage imputation methods: mice.impute.2l.2stage.bin Imputes univariate two-level binary variable from a logistic model. The imputation method is based on a two-stage estimator: at step 1, a logistic regression model is fitted to each observed cluster; at step 2, estimates obtained from each cluster are combined according to a linear random effect model. mice.impute.2l.2stage.norm Imputes univariate two-level continuous variable from a heteroscedastic normal model. The imputation method is based on a two-stage estimator: at step 1, a linear regression model is fitted to each observed cluster; at step 2, estimates obtained from each cluster are combined according to a linear random effect model. mice.impute.2l.2stage.pmm Similarly to mice.impute.2l.stage.norm, this function imputes univariate two-level continuous variable from a heteroscedastic normal model. The difference consists in replacing missing values by observed values instead of adding a parametric noise to the prediction of a linear model with random effects (as done in mice.impute.2l.stage.norm.mm and mice.impute.2l.stage.norm.reml). mice.impute.2l.2stage.pois Imputes univariate two-level count variable from a Poisson model. The imputation method is based on a two-stage estimator: at step 1, a Poisson regression model is fitted to each observed cluster; at step 2, estimates obtained from each cluster are combined according to a linear random effect model. The function find.defaultMethod The micemd package contains the function find.defaultMethod. With this function it is possible to get an advice about the imputation method on basis of the type of variables with missing data and the multilevel data structure. An example of how to apply this function is presented in Box 2 below. Insert Box 1 here. 6.10 Pooling Multilevel models 6.10.1 Pooling Multilevel Models in R After multilevel data has been imputed, pooling the result of a multilevel analysis in R is easy because you can make use of the pool function in the mice package. The following R code does the job. library(lme4) # lmer is used to specify the mixed model #fit &lt;-with(res.mice.md, lmer(Cholesterol ~ Time + SumSkinfolds + ( 1 | ID))) # summary shows the result in each imputed dataset #summary(fit) # pool gives the pooled result #summary(pool(fit)) 6.10.2 Pooling Multilevel Models in SPSS It is also possible to impute the missing data in R and then to transport the imputed datasets to SPSS. In SPSS you have to add a variable that distinguishes the imputed datasets and rename this variable into Imputation_. Than SPSS recognizes the dataset as a multiple imputed dataset. In SPSS there are two ways to run multilevel models. Both procedures can be found via Analyze -&gt; Mixed Models. One way is by using the Linear procedure for running Linear Mixed models when the outcome is continuous and the other is by using the Generalized Linear procedure for running Generalized Linear Mixed models when the outcome is continuous, dichotomous, etc. As is shown in (Figure 6.13), only for the Linear procedure the dataset is recognized as being a multiple imputed dataset because for that procedure the special mark before the word “Linear” is shown. Figure 6.13: The Mixed Linear menu. In other words, only with the Linear procedure pooled estimates can be generated and not for the Generalized Linear Mixed Models. This means that for example Multilevel logistic regression models cannot be pooled. The Mixed procedure in the imputed datasets will provide the following results (Figure 6.14) and (Figure 6.15). Figure 6.14: Table 7.2. Fixed effect parameters as a result of applying a linear mixed effects model in imputed datasets. Figure 6.15: Table 7.2. Fixed effect parameters as a result of applying a linear mixed effects model in imputed datasets. As is shown in (Figure 6.14), SPSS also provides the pooled covariance parameter estimates (in this case the pooled residual and random intercept variances). These pooled values are estimated using Rubin´s Rules. This information is not provided by the mice package. 6.11 Pooling GEE models 6.11.1 Pooling GEE Models in R Pooling of Generalized Estimating Equation (GEE) models is not possible with the pool function in the mice package, because this function requires the variance-covariance matrix of the GEE model. Other pooling procedures can be applied to get pooled results in R after you have imputed the data. One possibility is by using the MIextract and MIcombine functions in the mitools package. # Activate the geepack library to make use of the geeglm procedure library(geepack) # Run the GEE model in each imputed dataset #fit &lt;-with(res.mice.md, geeglm(Cholesterol ~ Time + SumSkinfolds, id=ID, # corstr = &quot;exchangeable&quot;)) # Activate the mitools package #library(mitools) # Extract the coefficients from the GEE model #gee.coeff &lt;- MIextract(fit$analyses , fun = coef) # Extract the sampling variances (standard errors squared) #var.gee &lt;- lapply(fit$analyses , FUN = function(x){ # gee.var &lt;- (summary(x)$coefficients[, &quot;Std.err&quot;])^2 # }) #summary(MIcombine(results=gee.coeff, variances=var.gee)) Another possibility is by using the pool_mi function that is available in the miceadds package. For this function the standard errors or the sampling variances can be used to get the pooled results. The results are comparable with the MIcombine function. # Extract the standard errors #se.gee &lt;- lapply(fit$analyses , FUN = function(x){ # gee.se &lt;- summary(x)$coefficients[, &quot;Std.err&quot;] # }) # Run the pool_mi function #summary(pool_mi(qhat=gee.coeff, se=se.gee)) 6.11.2 Pooling GEE Models in SPSS Generalized Estimating Equations (GEE) models can also be applied in SPSS over imputed datasets. The missing data can first be imputed in R and after that transported to SPSS. Then an imputation variable has to be added to the dataset and renamed into Imputation_ to separate the imputed datasets. Then, SPSS recognizes the dataset as a multiple imputed dataset. The GEE procedure in SPSS is accessible via Analyse -&gt; Generalized Linear Models -&gt; Generalized Estimating Equations ((Figure 6.16). Figure 6.16: Access of the Generalized Estimating Equations procedure via the Analyze menu. With this procedure also multilevel analyses can be conducted, although the number of levels is restricted if you compare this procedure with using Mixed models. When GEE is used in multiple imputed datasets, the following output will be generated (Figure 6.17). Figure 6.17: Parameter estimates as a result of applying a GEE model in multiple imputed datasets. "],
["background-information-to-pooled-statistical-methods.html", "Background information to Pooled Statistical Methods", " Background information to Pooled Statistical Methods "],
["rubins-rules.html", "Chapter 7 Rubin’s Rules 7.1 Pooling Effect estimates 7.2 Pooling Standard errors 7.3 Significance testing 7.4 Degrees of Freedom and P-values 7.5 Confidence Intervals", " Chapter 7 Rubin’s Rules Rubin´s Rules (RR) are designed to pool parameter estimates, such as mean differences, regression coefficients, standard errors and to derive confidence intervals and p-values. We illustrate RR with a t-test example in 3 generated multiple imputed datasets in SPSS. The t-test is used to estimate the difference in mean Tampascale values between patients with and without Radiation in the leg. The output of the t-test in the multiple imputed data is presented in Figure 7.1 and Figure 7.2. Figure 7.1: T-test for difference in mean Tampascale values between patients with and without Radiation in the leg applied in multiple imputed datasets. Figure 7.2: T-test for difference in mean Tampascale values between patients with and without Radiation in the leg applied in multiple imputed datasets. The result in the original dataset (including missing values) is presented in the row that is indicated by Imputation_ number 0. Results in each imputed dataset are shown in the rows starting with number 1 to 3. In the last row which is indicated as “Pooled”, the summary estimates of the mean differences and standard errors are presented. We now explain how these pooled mean differences and standard errors are estimated using RR. 7.1 Pooling Effect estimates When RR are used, it is assumed that the repeated parameter estimates are normally distributed. This cannot be assumed for all statistical test statistics, e.g. correlation coefficients. For these test statistics, transformations are first performed before RR can be applied. To calculate the pooled mean differences of Figure 7.1 the following formula is used (7.1): \\[\\begin{equation} \\bar{\\theta} = \\frac{1}{m}\\left (\\sum_{i=1}^m{\\theta_i}\\right ) \\tag{7.1} \\end{equation}\\] In this formula, \\(\\bar{\\theta}\\) is the pooled parameter estimate, m is the number of imputed datasets, \\(\\theta_i\\) means taking the sum of the parameter estimate (i.e. mean difference) in each imputed dataset i. This formula is equal to the basic formula of taking the mean value of a sequence of numbers. When we use the values for the mean differences in Figure 7.1, we get the following result for the pooled mean difference: \\[\\bar{\\theta} = \\frac{1}{3}(2.174 + 1.965+1.774)=1.971\\] 7.2 Pooling Standard errors The pooled standard error is derived from different components that reflect the within and between sampling variance of the mean difference in the multiple imputed datasets. The calculation of these components is discussed below. Within imputation variance This is the average of the mean of the within variance estimate, i.e. squared standard error, in each imputed dataset. This reflects the sampling variance, i.e. the precision of the parameter of interest in each completed dataset. This value will be large in small samples and small in large samples. \\[\\begin{equation} V_W = \\frac{1}{m}\\left (\\sum_{i=1}^m{SE_i^2}\\right ) \\tag{7.2} \\end{equation}\\] In this formula \\(V_W\\) is the within imputation variance, m is the number of imputed datasets, \\(SE_i^2\\) means taking the sum of the squared Standard Error (SE), estimated in each imputed dataset i. Using the standard error estimates in Figure 7.1, we get the following result: \\[V_W = \\frac{1}{3}(0.896^2 + 0.882^2 + 0.898^2)=0.7957147\\] Between imputation variance This reflects the extra variance due to the missing data. This is estimated by taking the variance of the parameter of interest estimated over imputed datasets. This formula is equal to the formula for the (sample) variance which is commonly used in statistics. This value is large when the level of missing data is high and smaller when the level of missing data is small. \\[\\begin{equation} V_B\\sqrt{\\frac{\\sum_{i=1}^m (\\theta_i - \\overline{\\theta})^2}{N-1} } \\tag{7.3} \\end{equation}\\] In this formula, \\(V_B\\) is the between imputation variance, m is the number of imputed datasets, \\(\\overline{\\theta}\\) is the pooled estimate, \\(\\theta_i\\) is the parameter estimate in each imputed dataset i. Using the mean differences in Figure7.1, we get the following result: \\[V_W = \\frac{1}{3-1}((2.174-1.971)^2+ (1.965-1.971)^2+(1.774-1.971)^2)=0.7957147\\] \\[V_B = \\frac{V_B}{m}\\] \\[\\begin{equation} V_{Total} = V_W + V_B + \\frac{V_B}{m} \\tag{7.4} \\end{equation}\\] \\[V_{Total} = 0.7957147+0.040027 + \\frac{0.040027}{3}\\] \\[SE_{Pooled} = \\sqrt{V_{Total}} = \\sqrt{0.849084} = 0.9214575\\] This value is equal to the (rounded) pooled standard error value of 0.921 in Figure 7.1. 7.3 Significance testing For significance testing of the pooled parameter, i.e. the mean difference in Figure 7.1, Formula (7.5) is used. This is the univariate Wald test (D. B. Rubin (1987), Van Buuren (2018), Marshall et al. (2009)). This test is defined as: \\[\\begin{equation} Wald_{Pooled} =\\frac{(\\overline{\\theta} - {\\theta_0})^2}{V_T} \\tag{7.5} \\end{equation}\\] where \\(\\overline{\\theta}\\) is the pooled mean difference and \\(V_T\\) is the total variance and is equal to the \\(SE_{Pooled}\\) (pooled standard error) that was derived in the previous paragraph, and \\(\\theta_0\\) is the parameter value under the null hypothesis (which is mostly 0). The univariate Wald test can be used to test all kind of univariate parameters of interest, such as mean differences and univariate regression coefficients from different tpe of regression models. The univariate Wald test in our example is calculated using the pooled regression coefficient and standard error from Figure 7.1: \\[Wald_{Pooled} = \\frac{1.971}{\\sqrt{0.849084}}=2.139\\] The univariate pooled Wald value follows a t-distribution. This distribution is used to derive the p-value. The value for t depends on the degrees of freedom, according to: \\[\\begin{equation} t_{df,1-\\alpha/2} \\tag{7.6} \\end{equation}\\] Where df are the degrees of freedom and \\(\\alpha\\) is the reference level of significance, which is usually set at 5%. The derivation of the degrees of freedom for the t-test is complex. There exist different formula´s to calculate the degrees of freedom and these are explained in the next paragraph. Because \\(t^2\\) is equal to F at the same number of degrees of freedom, we can also test for significance using a F-distribution, according to: \\[\\begin{equation} F_{1, df}=t^2_{df,1-\\alpha/2} \\tag{7.7} \\end{equation}\\] The degrees of freedom are equal to the degrees of freedom for the t-test above. 7.4 Degrees of Freedom and P-values The derivation of the degrees of freedom (df) and the p-value for the pooled t-test is not straightforward, because there are different formulas to calculate the df, an older and an adjusted version (Van Buuren (2018)). The older method to calculate the dfs results in a higher value for the df’s for the pooled result than the one in each imputed dataset. An example can be found in Figure 7.3. The degrees of freedoms are 148 in each imputed dataset (in the row for equal variances assumed, under the column df) and 507 for the pooled result. This is important to relize because different values for the df’s lead to different p-values. In SPSS the old way to calculate the dfs is used. Adjusted versions are used in the mice package for R. The differences between the older and adjusted methods to calculate the df’s is illustrated in more detail below. Figure 7.3: Part of Output of Figure 5.1. The value for the dfs are presented in the df column. The (older method) to calculate the df for the t-distribution is defined as (D. B. Rubin (1987), Van Buuren (2018)): \\[\\begin{equation} df_{Old} = \\frac{m-1}{lambda^2} = (m-1) * (1 + \\frac{1}{r^2}) \\tag{7.8} \\end{equation}\\] Where m is the number of imputed datasets and lambda is equal to the Fraction of Missing information (FMI), calculated by Formula (8.1) (Raghunathan (2016)), and r is the relative increase in variance due to nonresponse (RIV), calculated by Formula (8.2). The lambda value that is used in Formula (7.8) (and often used as alternative for the FMI) is not the same value for the fraction of missing information of 0.067 that SPSS presents in Figure 7.1. The FMI value of 0.067 is calculated by Formula (8.3) and is called FMI. When \\(df_{old}\\) is calculated with the information in Figure 7.3, we get: \\[df_{Old} = \\frac{3-1}{0.06283485^2} = 506.5576\\] This (rounded) value is equal to the df value in the row Pooled of 507 in (Figure 7.1). Formula (7.8) leads to a larger df for the pooled result, compared to the dfs in each imputed dataset, which is inappropriate. Therefore, Barnard and Rubin (1999) adjusted this df by using Formula (7.9): \\[\\begin{equation} df_{Adjusted} = \\frac{df_{Old}*{df_{Observed}}}{df_{Old}+{df_{Observed}}} \\tag{7.9} \\end{equation}\\] Where \\(df_{Old}\\) is defined as in Formula (7.8) and \\(df_{Observed}\\) is defined as: \\[\\begin{equation} df_{Observed} = \\frac{(n-k)+1}{(n-k)+3}*(n-k)(1-lambda) \\tag{7.10} \\end{equation}\\] Where n is the sample size in the imputed dataset, k the number of parameters to fit and lambda is obtained by Formula Formula (8.1). By filling in the formulas (7.10) and (7.9) we get for \\(df_{observed}\\) and \\(df_{adjusted}\\) respectively: \\[df_{Observed} = \\frac{(150-2)+1}{(150-2)+3}*(150-2)(1- 0.06283485)=136.8633\\] \\[df_{Adjusted} = \\frac{(506.5576* 136.8633)}{(506.5576+ 136.8633)}=107.7509\\] The number of 107.7509 is equal to the df used by mice. We can now derive the p-value for the pooled mean difference in the Tampascale between patients with and without Radiation in the leg. This two-sided p-value is: In SPSS: \\[t_{df,1-\\alpha/2}=2.139_{df{Old}}=0.03289185\\] In R: \\[t_{df,1-\\alpha/2}=2.139_{df{Adjusted}}=0.03467225\\] 7.5 Confidence Intervals For the 95% confidence interval (CI), the general formula can be used: \\[\\begin{equation} \\bar{\\theta} ± t_{df,1-\\alpha/2} * SE_{Pooled} \\tag{7.11} \\end{equation}\\] In this formula, \\(\\bar{\\theta}\\) is the pooled estimate, t is the t-statistic, df is degrees of freedom and \\(SE_{Pooled}\\) is the pooled standard error (Formula (7.4)). References "],
["measures-of-missing-data-information.html", "Chapter 8 Measures of Missing data information 8.1 Fraction of Missing Information - Lambda 8.2 Relative increase in variance 8.3 Fraction of Missing Information - FMI 8.4 Relative Efficiency", " Chapter 8 Measures of Missing data information These measures are the Fraction of Missing information (FMI), the relative increase in variance due to nonresponse and the Relative Efficiency. They are derived from values of the between, and within imputation variance and the total variance. There exist two versions of the FMI, which are referred to as lambda and FMI. 8.1 Fraction of Missing Information - Lambda The Fraction of Missing information, lambda, (Van Buuren (2018); Raghunathan (2016)) can be derived from the between and total missing data variance as: \\[\\begin{equation} Lambda = \\frac{V_B + \\frac{V_B}{m}}{V_T} \\tag{8.1} \\end{equation}\\] Where m is the number of imputed datasets and \\({V_B}\\) and \\({V_T}\\) are the between and total variance respectively. This value can be interpreted as the proportion of variation in the parameter of interest due to the missing data. When we use the \\({V_B}\\) and \\({V_T}\\) values that were calculated in paragraph 5.1.2, lambda will be: \\[Lambda = \\frac{0.040027 + \\frac{0.040027}{3}}{0.849084}=0.06283485\\] This specific value for the FMI, lambda, is not reported by SPSS, but is reported by the mice package in R. Confusingly, Van Buuren (2018) and Enders (2010) use the same formula to calculate this type of missing data information, but van Buuren calls it lambda and Enders FMI. 8.2 Relative increase in variance Another related measure is the relative increase in variance (RIV) due to nonresponse. This value is calculated as: \\[\\begin{equation} RIV = \\frac{V_B + \\frac{V_B}{m}}{V_W} \\tag{8.2} \\end{equation}\\] Where \\({V_B}\\) and \\({V_W}\\) are the between and within variance respectively. This value can be interpreted as the proportional increase in the sampling variance of the parameter of interest that is due to the missing data. Filling in this formula with the values for \\({V_B}\\) and \\({V_W}\\) from paragraph 5.1.2 results in: \\[Lambda = \\frac{0.040027 + \\frac{0.040027}{3}}{0.7957147}=0.06704779\\] This value is also presented in (Figure 7.1) in the column Relative Increase Variance. 8.3 Fraction of Missing Information - FMI \\[\\begin{equation} FMI = \\frac{RIV + \\frac{2}{df+3}}{1+RIV} \\tag{8.3} \\end{equation}\\] Where RIV is the relative increase in variance due to missing data and df is the degrees of freedom for the pooled result. The degrees of freedom for the pooled result can be obtained in two ways: \\({df_{Old}}\\) or \\({df_{Adjusted}}\\). In SPSS, FMI is calculated using \\({df_{Old}}\\), which results in: \\[FMI = \\frac{RIV + \\frac{2}{df+3}}{1+RIV}=\\frac{0.06704779 + \\frac{2}{506.5576+3}}{1+0.06704779}=0.0665132\\] In R package mice, FMI is calculated using the formula for \\({df_{Adjusted}}\\), that results in: \\[FMI = \\frac{RIV + \\frac{2}{df_{Adjusted}+3}}{1+RIV}=\\frac{0.06704779 + \\frac{2}{107.7509+3}}{1+0.06704779}=0.0797587\\] The difference between lambda and FMI is that FMI is adjusted for the fact that the number of imputed datasets that are generated is not unlimitedly large. These measures differ for a small value of the df. 8.4 Relative Efficiency The Relative Efficiency (RE) is defined as: \\[\\begin{equation} RE = \\frac{1}{1+\\frac{FMI}{m}} \\tag{8.4} \\end{equation}\\] FMI is the fraction of missing information and m is the number of imputed datasets. The RE value is only provided by SPSS and is calculated by filling in the values of (Figure 7.1) as follows: \\[RE = \\frac{1}{1+\\frac{0.0665132}{3}}=0.9783098\\] The RE gives information about the precision of the parameter estimate as the standard error of a regression coefficient. References "],
["pooling-correlation-coefficients-1.html", "Chapter 9 Pooling correlation coefficients 9.1 Pooled Wald test", " Chapter 9 Pooling correlation coefficients To pool correlation coefficients Fishers Z transformation is used. The following formulas are used (Raghunathan (2016), Van Buuren (2018) and Enders (2010)): \\[\\begin{equation} Z_i = \\frac{1}{2}ln\\frac{1+r_i}{1-r_i} \\tag{9.1} \\end{equation}\\] The \\({Z_i}\\) means the calculation of Fisher’s Z-value in each imputed dataset. Also, the variance of the correlation can be calculated using: \\[\\begin{equation} Var_Z=\\frac{1}{n-3} \\tag{9.2} \\end{equation}\\] n is the sample size in the imputed dataset. Now we can use Rubin’s Rules to calculate the Pooled correlation and variance. These values will be calculated with the transformed Z values. To obtain the pooled p-value for the correlation coefficient we use the formula: \\[\\begin{equation} Z=\\frac{Z_{Pooled}}{\\sqrt{Var_Z}} = \\frac{Z_{Pooled}}{\\frac{1}{\\sqrt{n-3}}}=Z_{Pooled}\\times\\sqrt{n_i-3} \\tag{9.3} \\end{equation}\\] In this formula z is the z-score and follows a standard normal distribution, \\(Z_{Pooled}\\) is the pooled Z transformation and \\(Var_Z\\) is the pooled variance. Finally, back transformation to the original scale of r is done by: \\[\\begin{equation} r_{Pooled} = \\frac{e^{2\\times\\\\Z_{Pooled}}-1}{e^{2\\times\\\\Z_{Pooled}}+1} \\tag{9.4} \\end{equation}\\] 9.1 Pooled Wald test The significance level for the pooled OR is derived by using the pooled Wald test. The pooled Wald test is calculated as: \\[Wald_{Pooled} =\\frac{-0.067}{0.046}=\\] This Wald pooled value follows a t-distribution with degrees of freedom (df) according to Formula 5.9. \\[df_{Old} = \\frac{m-1}{lamda^2}\\] For this Formula we need information of lambda, which is calculated as: \\[lambda = \\frac{V_B + \\frac{V_B}{m}}{lamda^2}\\] Using the values of the regression coefficients and standard errors, estimated in each imputed dataset of (Figure 4.12) we can calculate the following values for the between imputation and the total variance. \\[V_B= \\frac{(-0.090+0.067)^2 + (-0.061+0.067)^2 +(-0.051+0.067)^2}{2}=\\frac{0.000821}{2}=0.0004105\\] To calculate the total variance also the within imputation variance is needed. The within imputation variance can be calculated using Formula 5.2: \\[V_W= \\frac{0.039^2 + 0.040^2 + 0.039^2}{3}=0.001547333\\] The total variance becomes: \\[V_{Total} = 0.001547333+0.0004105+ \\frac{0.0004105}{3}=0.002094666\\] Now we can calculate lambda using: \\[lambda = \\frac{0.0004105 + \\frac{0.0004105}{m}}{0.002094666}=0.2612986\\] The lambda value is not presented by SPSS, but only in R using mice. Now we know the value for lambda, we can calculate the degrees of freedom to derive the p-value: \\[df_{Old} = \\frac{m-1}{lamda^2}=\\frac{2}{0.2612986^2}=29.29246\\] Which results in a p-value of: 0.1502045. References "],
["references.html", "References", " References "]
]
